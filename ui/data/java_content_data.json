[
  {
    "id": "1",
    "data": "<div class='entry-content'><p>Here we discuss basic concepts surrounding Java, they are terms and principles that surround java as a language.</p><h2><strong>JDK vs JRE</strong></h2><p><strong>JRE</strong> &#8211; Java Runtime Environment &#8211; environment to enable java applications to run.</p><p><strong>JDK</strong> &#8211; Java Development Toolkit &#8211; everything need to develop a java application, which in turns includes a runtime environment (JRE)</p><p><strong>JVM</strong> &#8211; Java Virtual Machine, is an abstract concept of the machine, it is required to run java applications. JRE is the actual implementation of the JVM with libraries etc.</p><h2><strong>Stack vs Heap</strong></h2><p><strong>Heap </strong>&#8211; objects created globally for the application resides in the heap. References are pointed inside the heap. <em>Strings are created here in the heap.</em></p><p><strong>Stack </strong>&#8211; are for short lived memory allocation, for example, variables created in a loop, or in a method that are no longer accessible once the method/loop ends.</p><p>(not my image)<img class='alignnone size-full wp-image-365' src='../../src/img/content/ss1.png' alt='' width='807' height='591' /></p><h2><strong>Just In Time Compiler</strong></h2><p>Java uses JIT Compiler or Just In Time Compiler, which runs after the program has started. Traditionally for an EXE file, it gathers all the necessary things before executing the application. In Contrast, Java has dynamic access to runtime information.</p><p>https://en.wikipedia.org/wiki/Just-in-time_compilation:</p><blockquote><p> The JIT compiler reads the bytecodes in many sections (or in full, rarely) and compiles them dynamically into machine language so the program can run faster. This can be done per-file, per-function or even on any arbitrary code fragment; the code can be compiled when it is about to be executed (hence the name &#8220;just-in-time&#8221;), and then cached and reused later without needing to be recompiled.</p></blockquote><p>&nbsp;</p></div>"
  },
  {
    "id": "2",
    "header": "Balanced Bracket Algorithm",
    "data": "<p>Typical problem scenario:</p> <ul> <li>For  every opening brace (i.e., <code>(</code>, <code>{</code>, or <code>[</code>), there is a matching closing brace (i.e., <code>)</code>, <code>}</code>, or <code>]</code>) of the same type (i.e., <code>(</code> matches <code>)</code>, <code>{</code> matches <code>}</code>, and <code>[</code> matches <code>]</code>). An opening brace must appear before (to the left of) its matching closing brace. For example, <code>]{}[</code> is <em>not balanced</em>.</li> <li>No unmatched braces lie between some pair of matched braces. For example, <code>({[]})</code> is <em>balanced</em>, but <code>{[}]</code> and <code>[{)]</code> are <em>not balanced</em>.</li> </ul> <p>Algorithm used:</p> <p><img class='alignnone size-full wp-image-182' src='../../src/img/content/algobalanced.jpg' alt='' width='1140' height='792' /></p> <p><strong>Possible implementation: </strong><br/><br/><img class='alignnone size-full wp-image-189' src='../../src/img/content/algobalanced2.png' alt='' width='662' height='623' /></p> </div>"
  },
  {
    "id": "3",
    "data": " <div class='entry-content'> <p><strong>Purpose</strong>: Comparable/Comparator are used to allow comparison of two or more objects. For example you have Person object and you want to add a natural ordering of each person by age. In other words to keep the order of each person from youngest to oldest or vice versa in a ordered collection like <a href='https://docs.oracle.com/javase/7/docs/api/java/util/TreeSet.html'>TreeSet</a> (There are other ordered collections for maps, list, sets as well)</p> <p><em>If you read the documentation on TreeSet it says it is using natural ordering. For primitive data types like String, integer, there is already a default natural ordering, (for numbers/integers it is ascending for String it using <a href='https://en.wikipedia.org/wiki/Lexicographical_order'>lexicographically ordering </a> (Dictionary ordering)), but you need to manually specify natural ordering for specific objects that you create, ie Person object:</em></p> <pre>public class Person{ private Integer age; //setters and getters }</pre> <h3><strong>Comparable</strong></h3> <ul> <li>Enables you to <strong>implement</strong> an <strong>interface</strong> from any of your class, forcing you to override the method <strong>compareTo()</strong> to enable you to compare this object to<strong> </strong>another instance of the object</li> <li>Comparable can be invoked with generic type invocation, for the case below we used <strong>Comparable</strong><em><strong>&lt;Person&gt;,</strong> </em>so that <em>compareTo()</em> method accepts Person as an parameter, otherwise it will accept object and you will have to typecast it to Person object.</li> <li>compareTo() method returns an int, so that: <ul> <li>if you return a negative integer = this Person object is less than the other Person</li> <li>if you return 0 = this person is equal to the other person</li> <li>if you return positive integer = this person is greater than the other person</li> <li>so if you are ordering by age</li> </ul> </li> </ul> <pre class='prettyprint'>public int compareTo(Person otherPerson) { \n if(this.age &gt; otherPerson.getAge()){ return 1; } \n else if(this.age == otherPerson.getAge()){ return 0; } \n else{ return -1; } }</pre> <p><strong>Example</strong>: (for simplicities sake, i&#8217;ve used Integer as data type and compared it using Integer&#8217;s compareTo method)</p> <pre class='prettyprint'>public class Person implements Comparable&lt;Person&gt;{ \n private Integer age; \n public int compareTo(Person otherPerson){ \n return this.age.compareTo(otherPerson.getAge()); } }</pre> <ul> <li>Once you have implemented Comparable, any object you add to any collection type that maintains natural ordering (<em>TreeSet, SortedSet, TreeMap</em>) will have automatically sorted your objects.</li> </ul> <h3><strong>Comparator</strong></h3> <ul> <li>Comparator uses a seperate <strong>class</strong> that enables you to compare two objects at a time.</li> <li>It has a method called <strong>compare(object o1, object </strong><strong>o2)</strong>, that enables you to compare two objects and returns int as well similar to comparable.</li> <li>Similar to the example above, we are sorting by age but notice that we had to create a new class called <strong>PersonComparator </strong>and then <strong>extend </strong>the class Comparator with the generic type invocation of Person.</li> <li>Comparator returns integer whereas, same as comparable: <ul> <li>if you return a negative integer = this Person object is less than the other Person</li> <li>if you return 0 = this person is equal to the other person</li> <li>if you return positive integer = this person is greater than the other person</li> </ul> </li> </ul> <p><strong>Example:</strong></p> <pre class='prettyprint'>public class PersonComparator implements Comparator&lt;Person&gt;{ \n public int compare(Person p1, Person p2){ return p1.getAge().compareTo(p2.getAge()); } }</pre> <p><strong>To use:</strong></p> <p>you would need to use a collection that can accept a comparator as a parameter, for examples sake I am using <a href='https://docs.oracle.com/javase/7/docs/api/java/util/LinkedList.html'>LinkedList </a></p> <pre class='prettyprint'>LinkedList&lt;Person&gt; personList = new LinkedList&lt;Person&gt;(); \n Person person1 = new Person(); \n person1.setAge(15); \n Person person2 = new Person(); \n person2.setAge(20); \n personList.add(person1); \n personList.add(person2); \n <strong>Collections.sort(personList, new PersonComparator());</strong></pre> <p>There other collections that accepts comparator as a constructor parameter, for example TreeSet:</p> <pre><strong>TreeSet&lt;Person&gt; personSet = </strong> <strong> new TreeSet&lt;Person&gt;(new PersonComparator());</strong></pre> <p><strong>Conclusion</strong></p> <p>Although both comparable and comparator provide same functionality it is more of a design <strong>whether you want to be implementing a interface or creating a new class.</strong></p> <p><strong> This can also be a matter of access to the class</strong>, if you are using a third party API, and you don&#8217;t have access to the Person object, then you are forced to use comparator instead.</p> <p>Also, a <strong>major hindrance</strong> for <strong>comparable</strong> is it is only limited to one implementation only, if you wanted to add more implementation for example, sort Person object by telephone number you can no longer use comparable, instead use <strong>comparator</strong> to create different implementations of different sortings.</p> <p><strong>Offical documentation: </strong></p> <ul> <li>https://docs.oracle.com/javase/7/docs/api/java/lang/Comparable.html</li> <li>https://docs.oracle.com/javase/7/docs/api/java/util/Comparator.html</li> </ul> </div>"
  },
  {
    "id": "4",
    "data": "<div class='entry-content'> <p>Basic Question in Java is usually what is the difference between Set/List/Map.</p> <p><strong>Set &#8211;</strong> is used usually used when you want to keep a collection of items that have no duplicates. Plain and simple. So think set when you want a unique collection of items.</p> <ul> <li>Variations <ul> <li>TreeSet (interface: SortedSet) &#8211; sorts the value in the set by natural ordering or comparator</li> <li>HashSet &#8211; no ordering, puts objects in different buckets based on their hash, faster than treeset</li> </ul> </li> </ul> <p><strong>List &#8211; </strong>is used with regards to just keeping a sequence of objects, it does not care about duplicates the primary focus of sequence and iteration</p> <ul> <li>Variations <ul> <li>ArrayList &#8211; basic list of items, random access, thread-safe</li> <li>Vector &#8211; non thread safe version of Arraylist</li> <li>Linked List &#8211; items are doubly linked to each other</li> </ul> </li> </ul> <p><strong>Map </strong><strong>&#8211; </strong>on the other hand has a key-value collection, you assign a key to a particular object, say you have 2 dog objects, you can assign different keys for each dog objects, like: <em>Brownie</em> and <em>Blackie</em>. So if you fetch from the collection <em>Blackie</em>, it will return that specific object associated to that key.</p> <ul> <li>Variations <ul> <li>TreeMap (interface SortedMap) &#8211; ordered version of map</li> <li>HashMap &#8211; no ordering of map, different buckets per object based on hash</li> <li>LinkedHashMap &#8211; doubly linked objects version of map</li> </ul> </li> </ul> <p>&nbsp;</p> </div>"
  },
  {
    "id": "5",
    "data": " <div class='entry-content'> <p>Static vs Instance variables</p> <p>When you declare a variable static, there is only one copy of that variable per class. Consider the following example (bad encapsulation code do not copy):</p> <p><img class='alignnone size-full wp-image-373' src='../../src/img/content/example1.png' alt='' width='547' height='446' /></p> <p>when we use static, there is only one copy per class, so no matter how many Humans objects we have, there is only one PLANET, notice we only changed planet for human 1 but the changes reflected to human 2 when we printed out the h2.PLANET.</p> <p>In contrast to Instance variables, they are unique per instance of the class, h1 has a different home compared h2.</p> <p>General Rules:</p> <ul> <li> A static variable is initialized when the JVM loads the class</li> <li>A <code>static method</code> <strong>cannot</strong> access <code>Non-static</code> variable or method</li> </ul> </div>"
  },
  {
    "id": "6",
    "data": "<div class='entry-content'> <p>There are instances where you would want to check memory allocation for your application in cases where your application sometimes randomly crashes. This is usually due to memory leaks in the application.</p> <p>Once you install JDK in your machine it comes with visualvm. It is just a matter connecting to it.</p> <p>Steps:</p> <p>1.) Make sure application server is running</p> <p>2.) go to visualvm:<img class='alignnone size-full wp-image-357' src='../../src/img/content/example2.png' alt='' width='744' height='510' /></p> <p>3.) You will see a pop-up window, under remote, key-in: localhost:8080 (or whatever port you are in)</p> <p>4.) Successfully logged in you will see, on the upper right you can perform a heap dump (current state of memory allocation and you will see which class are taking the most memory). On this dashboard you can monitor how the application is handling memory, threads, and Garbage Collection.</p> <p><img class='alignnone size-full wp-image-356' src='../../src/img/content/ss3.png' alt='' width='1913' height='960' /></p> <p>4.) Once you click heap dump you will be redirected to this page, at which point you can find the biggest objects in your application:</p> <p><img class='alignnone size-full wp-image-361' src='../../src/img/content/ss4.png' alt='' width='1712' height='795' /></p> <p>5.) you can also see in the classes tab which specific classes are eating up the memory</p> <p><img class='alignnone size-full wp-image-362' src='../../src/img/content/ss5.png' alt='' width='1643' height='815' /></p> <p>&nbsp;</p> </div>"
  },
  {
    "id": "7",
    "data": "<div class='entry-content'> <p><strong>Identifiers</strong></p> <ul> <li>identifiers begin with letter, underscore or currency</li> <li>After the first letters, identifiers can contain digits</li> <li>can be any length</li> <li>must be camelCase methods <ul> <li>Example of all valid identifier</li> </ul> </li> </ul> <pre>String b, _hello, $holder, number5, reallyreallyreallyreallylongName;</pre> <p><strong>Declaration Rules</strong></p> <ul> <li>source code file can only have one public class</li> <li>file name must match class name</li> <li>one package statement, with multiple imports <ul> <li>package must be first line</li> <li>import must come after</li> </ul> </li> <li>file can have more than one non public class</li> </ul> <p><strong>Class Access Modifiers</strong></p> <ul> <li>four access levels <ul> <li>public</li> <li>protected</li> <li>private</li> <li>default</li> </ul> </li> <li>Classes: public and default</li> <li>Class visibility revolves around whether another class can: <ul> <li>Create ans instance of another class</li> <li>extend another class</li> <li>access methods and variables of another class</li> </ul> </li> </ul> <p><strong>Non Access Class Modifiers</strong></p> <ul> <li>non access modifiers: final, abstract, or strictfp</li> <li>class cannot be both final and abstract &#8211; contradicting</li> <li>final class cannot be subclassed</li> <li>abstract cannot be instantiated</li> <li>single abstract method means the whole class must be abstract</li> <li>abstract class can have both concrete and abstract class</li> <li>first concrete class to extend abstract must implement all of its abstract methods</li> </ul> <p><strong>Interface</strong></p> <ul> <li>Interface are contracts for what a class can do</li> <li>interfaces are implemented by class</li> <li>interface can only have abstract methods</li> <li>interface methods are default public and default</li> <li>interface can have constants but they are implicity <em>public static and final</em></li> <li>A legal nonabstract implementing class has the following properties: <ul> <li> It provides concrete implementations for the interface&#8217;s methods.</li> <li> It must follow all legal override rules for the methods it implements.</li> <li> It must not declare any new checked exceptions for an implementation method.</li> <li> It must not declare any checked exceptions that are broader than the exceptions declared in the interface method.</li> <li>It may declare runtime exceptions on any interface method implementation regardless of the interface declaration.</li> <li>It must maintain the exact signature (allowing for covariant returns) and return type of the methods it implements (but does not have to declare the exceptions of the interface).</li> <li>Interfaces cannot extend a class, or implement a class or interface</li> <li>Interfaces can extend one or more other interfaces</li> <li><em>A class can extend only one class (no multiple inheritance), but it can implement many interfaces.</em></li> </ul> </li> </ul> <p>&nbsp;</p> <p>&nbsp;</p> </div>"
  },
  {
    "id": "8",
    "data": "<div class='entry-content'> <p><strong>Lambda functions &#8211; </strong>enables you to treat functionality as method arguments.</p> <ul> <li>The <em>forEach()</em> method for handling a collection of data. In this case, we have a list of Person, instead of doing a for loop we are using lambda syntax:</li> </ul> <blockquote><p><img class='alignnone size-full wp-image-244' src='../../src/img/content/ss5.png' alt='' width='488' height='268' /></p></blockquote> <p>For more complex methods you can wrap it in brackets, acting as anonymous classes.</p> <p><img class='alignnone size-full wp-image-246' src='../../src/img/content/ss6.png' alt='' width='568' height='173' /></p> <p>Lambda expressions can also be used to easily overwrite <strong>SAM</strong> interfaces, or Single Abstract Method interface. These are interfaces that have one sole method that the developer needs to overwrite. Example are <a href='https://docs.oracle.com/javase/7/docs/api/java/lang/Runnable.html'><em>Runnable</em> </a>and <a href='https://docs.oracle.com/javase/7/docs/api/java/util/Comparator.html'><em>Comparator</em> </a>interface.</p> <p>Instead of creating classes to overwrite theses interface, these can be easily handle by lambda expressions, the below will sort person by name:</p> <p><img class='alignnone size-full wp-image-247' src='../../src/img/content/ss7.png' alt='' width='730' height='83' /></p> <ul> <li>Similar to inner classes, lambda have access to it&#8217;s class variables</li> </ul> <p><img class='alignnone size-full wp-image-248' src='../../src/img/content/ss9.png' alt='' width='562' height='84' /></p> <p>Static Method references using lambda</p> <p><strong>Note</strong>: in this case my class name is <strong>Program</strong>, and I have a static method called <em><strong>processPeople</strong>(),</em> so looking at the below, for each person in my list, call the method <em><strong>processPeople</strong>(), </em>which can be your method to do validation and then save to the database.</p> <p><img class='alignnone size-full wp-image-249' src='../../src/img/content/ss10.png' alt='' width='507' height='156' /></p> <p>Instance Method References using lambda,</p> <p>in this case, we have a separate <strong><em>PersonDAO</em> </strong>class that has a method called <em><strong>saveToDB</strong>(), </em>we needed to initialize the class and called it <em>handler, </em>from there now you can say <strong><em>forEach</em> </strong>person in my list, use <em>handler</em> to <em><strong>saveToDB</strong>. </em>it is much simpler rather than using for loops or iterators to go through each value in your list.</p> <p><img class='alignnone size-full wp-image-250' src='../../src/img/content/ss11.png' alt='' width='412' height='182' /></p> <p>&nbsp;</p> </div>"
  },
  {
    "id": "9",
    "data": " <div class='entry-content'> <p>There are several changes for interfaces, including the addition of <strong>static</strong> and <strong>default</strong> methods. Interfaces in java 8 is more and more similar to abstract method. Consider the example below:</p> <p><img class='alignnone size-full wp-image-256' src='../../src/img/content/ss12.png' alt='' width='470' height='284' /></p> <p>Key notes:</p> <p><strong>static</strong></p> <ul> <li>static methods inside interfaces are only accessible inside the interface.</li> <li>No way to access from outside the interface, see <span style='text-decoration: underline;'><em>initializeTestData()</em></span><em> </em>method above.</li> </ul> <p><strong>default</strong></p> <ul> <li>default are concrete methods for interfaces, that can be or not overwritten</li> <li>this enables you to give default behaviour of interfaces without having to implement them, for example in this case, <em>testable</em> has a default behaviour of printing out some text.</li> <li><em>you can&#8217;t have multiple interfaces with same method that is implemented by one class see the below:</em></li> <li>both <em>Testable, Untestable have the same method test(), resulting in a compiler error</em></li> </ul> <p><em><img class='alignnone size-full wp-image-259' src='../../src/img/content/ss13.png' alt='' width='461' height='387' /></em></p> <p><strong>the code above will require you to provide a test method in your class.</strong></p> <h2>Lambda and Functional Interfaces</h2> <p>There is a new of interaction for lambda and interfaces. First of, what are <strong><span style='text-decoration: underline;'><em>functional interfaces</em></span></strong>? These are interfaces that only have one sole method. A great example of this is the interface <em>Runnable, </em>when you want to run threads. Another interface well known is <em>Comparable, </em>for when you want to compare objects.</p> <p>Now with lambda you can instantiate functional interfaces instantly, consider the below:</p> <p><img class='alignnone size-full wp-image-260' src='../../src/img/content/ss14.png' alt='' width='547' height='259' /></p> <p><strong>Key Notes:</strong></p> <ul> <li>Notice the syntax for <em><span style='text-decoration: underline;'>R</span><span style='text-decoration: underline;'>unnable</span></em>, whatever we pass inside the curly brackets, will automatically be the implementation of the single method in the interface.</li> <li>You can create your own functional interface, and have parameters as well, see syntax for <em><span style='text-decoration: underline;'>Bounceable</span></em></li> <li>Functional interfaces cannot have more than one method, otherwise lambda expressions will not know which method to use</li> <li>There is a new package in java that is solely containing functional interfaces for lambda expressions, will look into that in the next post.</li> </ul> <p>&nbsp;</p> </div>"
  },
  {
    "id": "10",
    "data": "<div class='entry-content'> <p>As in previous topic, we can use functional interfaces to use in lambda expression see topic on: Java 8 interfaces</p> <p>However Java 8 goes on more to provide us mechanism to easier create lambda expressions. See <a href='https://docs.oracle.com/javase/8/docs/api/java/util/function/package-summary.html'>java.util.function library</a>. No need to create an functional interface and just overwrite existing ones in this library.</p> <p>This library can be divided in to four sections:</p> <h2><strong>Consumers</strong></h2> <p>Consumers are interfaces that focuses on accepting parameters. for example: <a href='https://docs.oracle.com/javase/8/docs/api/java/util/function/BiConsumer.html'>BiConsumer</a> &#8211; as the api reads, this accepts two arguments and produces no result:</p> <p><img class='alignnone size-full wp-image-269' src='../../src/img/content/ss15.png' alt='' width='602' height='177' /></p> <p>The same logic applies to all the *<strong>Consumer</strong> in the library like <a title='interface in java.util.function' href='https://docs.oracle.com/javase/8/docs/api/java/util/function/DoubleConsumer.html'>DoubleConsumer</a>, <a href='https://docs.oracle.com/javase/8/docs/api/java/util/function/LongConsumer.html'>LongConsumer</a>, and so forth.</p> <h2><strong>Functions</strong></h2> <p>Functions are interfaces that are focused on doing accepting a parameter and returning a result. for example: <a href='https://docs.oracle.com/javase/8/docs/api/java/util/function/BiFunction.html'>BiFunction </a>&#8211; as the api reads, accepts two arguments and returns one result we can use it as below:</p> <p><img class='alignnone size-full wp-image-270' src='../../src/img/content/ss-10.png' alt='' width='521' height='154' /></p> <p>The same logic applies to all the *<strong>Function</strong> in the library like <a title='interface in java.util.function' href='https://docs.oracle.com/javase/8/docs/api/java/util/function/DoubleToLongFunction.html'>DoubleToLongFunction</a>, <a title='interface in java.util.function' href='https://docs.oracle.com/javase/8/docs/api/java/util/function/LongToDoubleFunction.html'>LongToDoubleFunction</a>, and so forth.</p> <h2><strong>Supplier</strong></h2> <p>In contrast, <a href='https://docs.oracle.com/javase/8/docs/api/java/util/function/Supplier.html'>Supplier </a>methods just returns an object without an parameters: <br/> <img class='alignnone size-full wp-image-271' src='../../src/img/content/ss16.png' alt='' width='508' height='94' /></p> <p>This applies to all *<strong>Supplier</strong> methods, like <a href='https://docs.oracle.com/javase/8/docs/api/java/util/function/IntSupplier.html'>IntSupplier</a>, <a href='https://docs.oracle.com/javase/8/docs/api/java/util/function/BooleanSupplier.html'>BooleanSupplier</a>, and so forth.</p> <h2><strong>Operator</strong></h2> <p>Finally we have <a href='https://docs.oracle.com/javase/8/docs/api/java/util/function/UnaryOperator.html'>UnaryOperator </a>methods in the library, it is much similar to consumer, but in a way it just accepts an object and returns that same type.</p> <p>The best way to use UnaryOperators are for when using objects you have created, see example below.</p> <p>In this case I have created a People class, and the output will also be the same class:</p> <br/><p><img class='alignnone size-full wp-image-272' src='../../src/img/content/ss17.png' alt='' width='491' height='482' /></p> <p>Same applies to other *<strong>Operators</strong> method in the library however the others have a predefined data type. for example <a href='https://docs.oracle.com/javase/8/docs/api/java/util/function/DoubleUnaryOperator.html'>DoubleUnaryOperator</a>, <a href='https://docs.oracle.com/javase/8/docs/api/java/util/function/IntUnaryOperator.html'>IntUnaryOperator</a>, and so forth.</p> </div>"
  },
  {
    "id": "11",
    "data": " <div class='entry-content'> <p><strong>Java 8 Streams</strong></p> <p>Streams provide another layer of abstraction on our collections of data that enable you to perform complex sets of actions on. The most powerful use of streams are chaining commands. For example, no longer do you need to perform a for loop to iterate over a collection of String. Stream supports aggregate operations like filter, map, limit, reduce, find, match, and so on.</p> <h3><strong>Filter</strong></h3> <p>Filter is mainly used to eliminate elements based on a criteria, the parameter it takes comes in the form of a <a href='https://docs.oracle.com/javase/8/docs/api/java/util/function/Predicate.html'>Predicate</a> (takes in one parameter, returns boolean). Below we have a list of String and we only to print out those starting with &#8216;Good&#8217;, so below we chained to methods, <em>filter</em>(), and .<em>forEach</em>()<img class='alignnone size-full wp-image-292' src='../../src/img/content/ss1.png' alt='' width='719' height='85' /></p> <h3><strong>Collectors</strong></h3> <p>In the previous example, we just passed the stream result to another function forEach to print out whatever was the result of the <em>filter</em> command. What do we do if we want to return a mutable list of what we have filtered? Collectors are exactly for that.</p> <p>Continuing on the previous example, after we filtered, we can now return another list of Strings in <em>filteredData</em> object:</p> <p><img class='alignnone size-full wp-image-296' src='../../src/img/content/ss18.png' alt='' width='712' height='62' /></p> <p>There are other powerful methods that Collectors can do, but we won&#8217;t dive in too much detail for that. For Example, you can comma-separate your results, pass them to another method, or even map them to maps. For more details check out this documentation: <a href='https://docs.oracle.com/javase/8/docs/api/index.html?java/util/stream/Collector.html'>Collectors</a>.</p> <h3><strong>Limit</strong></h3> <p>Limit is quite straight-forward, it just limits the number of elements inside your stream. This method takes one parameter, just a long object that specifies the actual limit.</p> <h3><strong>Sorted</strong></h3> <p>Sorted is also quite straight-forward, it sorts the elements inside the stream. There are two possible implementation, no parameters, which assumes that object used in inside your stream have their own comparable implementation. The other takes a comparator parameter, with your implementation, here is both limit and sort in action:</p> <p><img class='alignnone size-full wp-image-297' src='../../src/img/content/ss19.png' alt='' width='333' height='49' /></p> <h3><strong>Map</strong></h3> <p>Map enables us to perform an operation on each individual element in the stream. Gone are the days for multiple for-loops when you start using lambdas. Check out this single line below which is based on the previous examples:</p> <p><img class='alignnone size-full wp-image-298' src='../../src/img/content/ss20.png' alt='' width='658' height='22' /></p> <p>The example, will append my name on all the greeting, so it will be &#8220;Hello Mel&#8221;, &#8220;Bye Mel&#8221;, and so on. <strong>All this in just one line of code!</strong></p> </div>"
  },
  {
    "id": "12",
    "data": " <div class='entry-content'> <p>Hibernate Class:</p> <pre class='prettyprint'>@Entity @Table(name='Person') \n public class Person{ \n @OneToMany(mappedBy = 'children', cascade = CascadeType.ALL, orphanRemoval = true) \n private Set&lt;Child&gt; children; \n ... //setters and getters }</pre> <p>&nbsp;</p> <p>Error occurs when in controller you are trying to set it to null instead of clearing the set/list, the following pseudo code will throw an exception as “A collection with cascade=”all-delete-orphan” was no longer referenced by the owning entity instance”</p> <pre class='prettyprint'>public String deleteChildren(HttpServletRequest request, @ModelAttribute('person') Person person, BindingResult result, Model model) { \n person.setChildren(null); \n repository.save(person); \n}</pre> <p>instead use:</p> <pre>person.getChildren.clear(); repository.save(person);</pre> </div>"
  },
  {
    "id": "13",
    "data": "<div class='entry-content'> <p>How to Map Enum to hibernate class:</p> <pre class='prettyprint'>@Entity @table('d_user_group') \n public class UserGroup{ \n @Enumerated(EnumType.STRING) \n @Column(name='user_group_status') \n private UserGroupStatusEnum userGroupStatus; \n public enum UserGroupStatusEnum{ \n Active, Draft; \n } \n }</pre> <p>In the case above we have a table<em> d_user_group</em>, that will have a column <em>user_group_status</em>, that only accepts <em>Enum</em> as a parameter. If you specify <em>EnumType.String</em> it will store the actual words from your Enum:</p> <p class='fix-link-focus'><img src='../../src/img/content/ss22.png'/></p> <p>Selecting <em>EnumType.Ordinal</em> in contrast will store Integer based on ordering, so Active will be 0, Draft will be 1, and so on.</p> </div>"
  },
  {
    "id": "14",
    "data": " <div class='entry-content'> <p><strong>EC2 &#8211; Elastic Cloud Compute</strong></p> <p><strong><span style='text-decoration: underline;'>Terms:</span></strong></p> <ul> <li><strong>EC2 Instances</strong> &#8211; Virtual Computing Environments</li> <li><strong>AMI</strong> (Amazon Machine Images) &#8211; preconfigured EC2 templates for your instances. ie. can be SQL server, Bastion, NAT instances etc</li> <li><strong>Instance Types</strong> &#8211; Processing power of your EC2 configurations. ie. CPU, memory, storage, network capacity are separated as Instance Types</li> <li><strong>Key pairs </strong>&#8211; Secure login information for your instances</li> <li><strong>EBS (Amazon Elastic Block Store) &#8211; </strong>persistent storage volumes for your EC2</li> <li><strong>Region/AZ (Availability Zone) &#8211; </strong>Various physical locations of your resources</li> <li><strong>Elastic IP Address</strong> &#8211; static IP address for dynamic cloud computing</li> </ul> <p><span style='text-decoration: underline;'><strong>Instances Types:</strong></span></p> <p><strong>to remember all the types easily here is an acronym: DRMCGFTPX</strong> (<strong>think: Doctor Mac Gift Pix, credit to acloudguru for acronym</strong>)</p> <p><strong>D</strong>(ense disk)<strong> R</strong>(AM memory-intensive)<strong> M</strong>(icro)<strong> C</strong>(ompute for processsing)<strong> G</strong>(raphics) <strong>I</strong>(nput/Output) <strong>F</strong>(pga or field programmable arrays )<strong> T</strong>(2 micro)<strong> P</strong>(ics)<strong> X</strong>(treme)</p> <p><span style='text-decoration: underline;'><strong>Pricing Model:</strong></span></p> <ul> <li><strong>On &#8211; Demand</strong> :pay only what you request and what you use, no up-front fees</li> <li><strong>Spot : </strong>flexible rates, you bid for what price you are willing to pay, somewhat sort of stock market for ec2 instances</li> <li><strong>Reserved : </strong>Pay upfront for a period you want to pay for your instance, and in return you get a significant discount depending on which you are signing up for. (longer period and more instance = more discount) <ul> <li> <strong>Payment options:</strong> All Upfront, Partial Upfront, and No Upfront.</li> <li><strong>Can I move a reserved instance from one region to another? No</strong></li> </ul> </li> </ul> <p>Access to EC2 can be the following:</p> <ul> <li>EC2 Console</li> <li>AWS Web API Call</li> <li>AWS SDK</li> <li>AWS Command Line interface</li> </ul> <p><span style='text-decoration: underline;'><strong>Amazon Machine Image (AMI)</strong></span></p> <ul> <li>Preconfigured Virtual servers</li> <li>auto-assign public IP</li> <li>Termination protection off by default (you can delete the image by clicking on delete action if termination protection is turned on)</li> <li>Root device by default is not encrypted (there are ways to encrypt it)</li> <li>EBS backed &#8211; default is delete the volume with instance</li> </ul> <p><span style='text-decoration: underline;'><strong>Placements Groups</strong> </span><em>&#8211; </em>AWS feature that enables EC2 instances (usually of similar instance types) to communicate with each other if in the same Availability Zone with high bandwith and low latency. Restrictions: Unique per AWS account, can&#8217;t move existing EC2 to placement groups, one AZ per group.</p> <p><img class='alignnone size-full wp-image-279' src='../../src/img/content/ss23.png' alt='ss1.PNG' width='517' height='323' /></p> <p><strong>Volume Storage: </strong>Will be discussed in another entry</p> <p><span style='text-decoration: underline;'><strong>Additional Features to be noted</strong></span></p> <p><strong>CloudWatch</strong></p> <ul> <li>Enables performance monitoring</li> <li>set on EC3 creation</li> <li>Standard config gives you statuse very 5 mins vs Detailed config gives you per minute but with additional charges</li> <li>Has Dashboards, Events, Alarms, Logs</li> </ul> <p><strong>AutoScaling</strong></p> <ul> <li>Enables you to scale your EC2, to increase/decrease in number depending on load and availability</li> <li>It checks health status via Elastic Load Balancer</li> </ul> <p><strong>Elastic Load Balancer</strong></p> <ul> <li>Health check for your instances</li> <li>Gives you ability for Cross Zone load balancing</li> <li>No Ip given, only DNS</li> </ul> <p><strong>Security Group</strong></p> <ul> <li>Gives you ability to control inbound and outbound traffic , ie enable port 8080, 1433 (sql ), 443 (https), 22 (SSH)</li> <li><strong>Stateful by default</strong>, if you enable inbound port 8080 it is immediately given outbound as well</li> </ul> </div>"
  },
  {
    "id": "15",
    "data": "<div class='entry-content'> <p><strong>Autoscaling</strong> &#8211; ensures you have correct number of Ec2 to handle load of your application, whether to scale up or scaled down. AutoScaling groups are the cornerstone of any self-healing application on AWS.</p> <p>Auto scaling <strong><em>is not really intended to respond to instantaneous spikes in traffic</em></strong>, as it will take some time to spin-up the instances that will handle the additional traffic.</p> <p><strong>Pricing</strong> &#8211; Autoscaling is FREE</p> <p><strong>Benefits:</strong> you can scale your resource depending on the demand of your application, hence you only pay for what you actually use, instead for example allocating too much memory on your servers but peak usage only occurs on certain days, as per below:</p> <p><img class='alignnone size-full wp-image-747' src='../../src/img/content/ss24.png' alt='ss1.PNG' width='456' height='296' /></p> <p><strong>Limitation</strong></p> <ul> <li>Autoscale can only be in on one region, but can have multiple Availability zones</li> <li>Autoscale does rebalancing activities to make sure scaling is balanced among AZs this occurs under the following conditions <ul> <li>you issue a request to change AZ for your group</li> <li>explicit call for termination of instance</li> <li>AZ had insufficient capacity recovers</li> <li>Spot market price</li> </ul> </li> <li>Autoscale launches new instances before termination old ones</li> </ul> <p><strong>Scale Out Occurances:</strong></p> <ul> <li>Manual Scaling</li> <li>Dynamic Scaling</li> <li>Scheduled Scaling</li> </ul> <p><strong>Scale In Occurances:</strong></p> <ul> <li>Manually decrease size</li> <li>create scaling policy to decrease size depending on demand</li> <li>scheduled scale</li> </ul> <p><strong>Autoscaling cooldowns &#8211;</strong> ensures that auto scaling does not launch or terminate additional instance before previous scaling takes effect. (does not apply if instance becomes unhealthy). Default cooldown period is <strong>300 seconds</strong></p> <p><strong>Cooldown periods are not supported for step scaling policies or scheduled scaling.</strong></p> <p><strong>Default termination policy </strong></p> <ul> <li>Check if there are any instances in multiple AZ <ol> <li>if Yes select AZ with most instance</li> <li>if No select instance with oldest config &#8211; TERMINATE</li> </ol> </li> <li>Are there multiple instance with oldest config <ol> <li>if yes select instance with most closest to billing hour</li> <li>if No Terminate</li> </ol> </li> <li>Are there multiple instance closest to billing hour <ol> <li>if yes select at random then terminate</li> <li>if no terminate</li> </ol> </li> </ul> <p><strong>Possible termination policies:</strong></p> <ul> <li>OldestInstance</li> <li>newInstance</li> <li>oldestLaunchConfiguration</li> <li>closestToNextInstanceHour</li> <li>Default</li> </ul> <p><strong>Instance Protection &#8211; protects an instance from getting deleted</strong></p> <p><strong>Lifecycle Hooks</strong> &#8211; events you want to trigger when autoscaling occurs, ie create notification or perform some lambda function</p> <p><strong>Standby State &#8211;</strong> you can put an instance on standby state, meaning it is still part of the scaling group but it does not handle application, this is useful for example if you want to upgrade application and just put the instance on standby while others handle the traffic</p> </div><div class='entry-content'> <p><strong>Autoscaling</strong> &#8211; ensures you have correct number of Ec2 to handle load of your application, whether to scale up or scaled down. AutoScaling groups are the cornerstone of any self-healing application on AWS.</p> <p>Auto scaling <strong><em>is not really intended to respond to instantaneous spikes in traffic</em></strong>, as it will take some time to spin-up the instances that will handle the additional traffic.</p> <p><strong>Pricing</strong> &#8211; Autoscaling is FREE</p> <p><strong>Benefits:</strong> you can scale your resource depending on the demand of your application, hence you only pay for what you actually use, instead for example allocating too much memory on your servers but peak usage only occurs on certain days, as per below:</p> <p><img class='alignnone size-full wp-image-747' src='../../src/img/content/ss24.png' alt='ss1.PNG' width='456' height='296' /></p> <p><strong>Limitation</strong></p> <ul> <li>Autoscale can only be in on one region, but can have multiple Availability zones</li> <li>Autoscale does rebalancing activities to make sure scaling is balanced among AZs this occurs under the following conditions <ul> <li>you issue a request to change AZ for your group</li> <li>explicit call for termination of instance</li> <li>AZ had insufficient capacity recovers</li> <li>Spot market price</li> </ul> </li> <li>Autoscale launches new instances before termination old ones</li> </ul> <p><strong>Scale Out Occurances:</strong></p> <ul> <li>Manual Scaling</li> <li>Dynamic Scaling</li> <li>Scheduled Scaling</li> </ul> <p><strong>Scale In Occurances:</strong></p> <ul> <li>Manually decrease size</li> <li>create scaling policy to decrease size depending on demand</li> <li>scheduled scale</li> </ul> <p><strong>Autoscaling cooldowns &#8211;</strong> ensures that auto scaling does not launch or terminate additional instance before previous scaling takes effect. (does not apply if instance becomes unhealthy). Default cooldown period is <strong>300 seconds</strong></p> <p><strong>Cooldown periods are not supported for step scaling policies or scheduled scaling.</strong></p> <p><strong>Default termination policy </strong></p> <ul> <li>Check if there are any instances in multiple AZ <ol> <li>if Yes select AZ with most instance</li> <li>if No select instance with oldest config &#8211; TERMINATE</li> </ol> </li> <li>Are there multiple instance with oldest config <ol> <li>if yes select instance with most closest to billing hour</li> <li>if No Terminate</li> </ol> </li> <li>Are there multiple instance closest to billing hour <ol> <li>if yes select at random then terminate</li> <li>if no terminate</li> </ol> </li> </ul> <p><strong>Possible termination policies:</strong></p> <ul> <li>OldestInstance</li> <li>newInstance</li> <li>oldestLaunchConfiguration</li> <li>closestToNextInstanceHour</li> <li>Default</li> </ul> <p><strong>Instance Protection &#8211; protects an instance from getting deleted</strong></p> <p><strong>Lifecycle Hooks</strong> &#8211; events you want to trigger when autoscaling occurs, ie create notification or perform some lambda function</p> <p><strong>Standby State &#8211;</strong> you can put an instance on standby state, meaning it is still part of the scaling group but it does not handle application, this is useful for example if you want to upgrade application and just put the instance on standby while others handle the traffic</p> </div>"
  },
  {
    "id": "16",
    "data": "<header class='entry-header'> <h1 class='entry-title'>EBS vs EFS – Amazon WebServices</h1> </header> <div class='entry-content'><strong>Elastic Block Store </strong> <ul> <li>provides highly available reliable storage volumes</li> <li>Automatically stores in multiple volumes in same availability zone as part of normal operation</li> <li>1 EC2 : can have multiple EBS, not vice versa</li> <li>Long term persistent</li> <li>Simplified data encryption</li> <li><strong>Types</strong> <ul> <li>General Purpose SSD (gp2) – 3000 IOPS <ul> <li>max size 16Tib</li> </ul> </li> <li>Provisioned IOS SSD – 20,000 IPS</li> <li>Throughput Optimized HDD – low cost magnetic storage</li> <li>Cold HDD – lowest cost, lowest performance</li> </ul> </li> <li>EBS volumes are created in a specific Availability Zone, and can then be attached to any instances in that same Availability Zone, need to copy if it is in another region</li> <li>Performance metrics, such as bandwidth, throughput, latency, and average queue length</li> <li>is a block level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest-latency access to data from a single EC2 instance.</li> <li> The data from an EBS volume snapshot is durable because EBS snapshots are stored on the Amazon S3-Standard</li> <li><strong>EBS Volumes cannot be attached to an EC2 instance in another AZ.</strong></li> <li> Stopping an EBS-backed on-demand instance, will stop the charges and preserve the data.</li> <li><strong>When a snapshot is being taken against an EBS volume, the volume is still available, and instance can still communicate with it</strong></li> <li><strong>EC2 that are EBS backed only: T2, C4</strong></li> </ul> <strong>EFS (Elastic Files System)</strong> <ul> <li>scalable file storage for EC2</li> <li>you can attach to multiple EC2</li> <li>up to petabytes</li> <li>is a file storage service for use with Amazon EC2. Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently-accessible storage for up to thousands of Amazon EC2 instances.</li> </ul> </div>"
  },
  {
    "id": "17",
    "data": "<h1>Amazon Relational Database Service</h1> Services: <ul> <li>RDS Databases covered: <ul> <li>Maria</li> <li>Oracle</li><code></code> <li>SQL Server</li> <li>Postgre SQL</li> <li>Aurora</li> <li>MySQL</li> </ul> </li> <li>Dynamo DB – NO SQL databases <ul> <li>MemCache</li> <li>Redis</li> </ul> </li> <li>Redshift – Data Warehousing</li> <li>Database Migration Service (DMS)</li> </ul> Types of Processing: <ul> <li>Online Transaction Processing (OLTP) <ul> <li>Standard processing</li> <li>Transactional data ie get one order detail</li> </ul> </li> <li>Online Analytics process (OLAP) <ul> <li>more complex computation</li> <li>query is done on a copy as to not disturb production</li> </ul> </li> </ul> <strong>Database Backups:</strong> <ul> <li>Automated Backup <ul> <li>Retention period – default is 7 days, up to 35 days</li> <li>saved on S3</li> <li>on by default</li> </ul> </li> <li>DB-Snapshots – manually turned on</li> <li>Encryption at rest supported using KMS</li> <li>Encrypting existing DB currently not supported</li> <li> <div>In RDS, changes to the backup window take effect Immediately</div></li> </ul> <strong>Multi Availability Zone Replication</strong> – replicate DB to another AZ for DR not for performance, this enables auto failover from geographic location to another.<strong>Exam NOTES:</strong> <ul> <li>Dynamo DB can scale on the fly, RDS cannot.</li> <li>RDS can scale on read not on write (Read Replica)</li> <li>You CANNOT RDP or SSH to RDS instance</li> <li>At the present time, encrypting an existing DB Instance is not supported. To use Amazon RDS encryption for an existing database, create a new DB Instance with encryption enabled and migrate your data into it. (manually import data)</li> </ul> Read Replica on RDS: <ul> <li>Replicate RDS on different RDS</li> <li>Async replication is done</li> <li>Replicas are only read copies, and each EC2 instance can connect to a replica</li> <li>Automated backup is required for read replica</li> <li>replicas can have replicas (this will have performance implications)</li> <li>NO multi AZ</li> <li>replicas can be promoted to their own DBs</li> </ul> <h2><strong>Dynamo DB features </strong> – No SQL services from AWS</h2> <ul> <li>consistent latency on scale</li> <li>stored on SSD (store sdata in partition)</li> <li>3 geographic locations</li> <li>Eventual consistency (1 second), strong consistent read (</li> <li>cheaper on read than write</li> <li>push button scaling</li> <li> DynamoDB is automatically redundant across multiple availability zones.</li> <li>terms: <ul> <li><strong>Tables</strong> – similar to RDS</li> <li><strong>Items</strong> – similar to columns, items is a group of attributes that represent one object in a table, ie a person table, each item would represent one person <ul> <li>400 KB limit on item size</li> </ul> </li> <li><strong>Attributes</strong> – each item are composed attribute ie. FirstName, LastName, Age etc</li> </ul> </li> <li><strong>Primary Key</strong> <ul> <li><strong>Partition Key –</strong> simple PK hash function to identify items</li> <li><strong>Partition key and sort key</strong> – composite pk which parition key as above and sort key which is sorted by sort key value</li> </ul> </li> <li>API : Control plane (CRUD tables), Data plane (CRUD Data), DynamoDb Streams</li> <li>Data Types – Scalar (number, String, binary, boolean, null) Document (JSON document [list/map]), Set Type (String set, number set, binary set)</li> <li><strong>Read Consistency</strong> <ul> <li>Eventually Consistent Reads</li> <li>Strongly Consistent Reads</li> </ul> </li> <li><strong>Provisioned throughput: 1 read unit = 4kb, 1 write unit 1kb</strong></li> <li><strong>Batch Operations</strong>: you can get up to 16mb of data, which can be as many as 100 items</li> <li><strong>Conditional Writes</strong> – prevents concurrent overwrite of same row by using <strong>ReturnConsumedCapacity</strong></li> <li><strong>Scan – </strong>as opposed to query, scan returns all of the data attributes for every item in a table or index, maximum of 1mb <ul> <li>Scan uses sequential scanning, you can use parellel scan by using segments for faster retrieval of large amounts of data</li> </ul> </li> </ul> <strong>Redshift</strong><strong> – </strong>Data warehousing mechanism from AWS <ul> <li>Typical database block sizes range from 2 KB to <b>32 KB</b>. Amazon Redshift uses a block size of <b>1 MB</b></li> <li><strong>Single node</strong> can be up to <strong>160gb</strong></li> <li><strong>multinodes</strong> can contain up to <strong>128 nodes</strong> <ul> <li><strong>leader node</strong>– receives query</li> <li><strong>compute node</strong> – performs query <ul> <li>Node slices – nodes are partitioned into slices, each slice a portion of memory and disk space is allocated to do the workload</li> </ul> </li> </ul> </li> </ul> <strong>Performance</strong>: <ul> <li>Columnar Data Storage – column based storage (RDS is row based)</li> <li>Can handle large data set with fewer I/O</li> <li>Advanced Data Compression – 10 times faster than RDS <ul> <li>columns are all the same type hence it is faster than RDS</li> <li>no indexes/views</li> </ul> </li> <li>Massive Parallel Processing (MPP)</li> <li>Query optimizer</li> <li>Compiled Code – leader node compiles the code already eliminating overhead of interpreting code and distributes across all nodes</li> </ul> <strong>ElastiCache</strong><strong> </strong> – Caching mechanism of AWS<strong>Features</strong>: <ul class='itemizedlist' type='disc'> <li class='listitem'>Automatic detection and recovery from cache node failures.</li> <li class='listitem'>Automatic failover (Multi-AZ) of a failed primary cluster to a read replica in Redis replication groups.</li> <li class='listitem'>Flexible Availability Zone placement of nodes and clusters.</li> <li class='listitem'>Integration with other AWS services such as Amazon EC2, CloudWatch, CloudTrail, and Amazon SNS to provide a secure, high-performance, managed in-memory caching solution.</li> </ul> <strong>Engines</strong> <ul> <li>MemCached <ul> <li>memory object caching</li> <li>not AZ</li> </ul> </li> <li>Redis <ul> <li>key/value pair storing</li> <li>sets/list</li> <li>master – slave replicatin</li> <li>AZ</li> </ul> </li> </ul> <strong>AuroraDB </strong>– Amazon created SQL, this is always recommended when using AWS cloud computing database <ul> <li>scaling capability</li> <li><strong>3 Az with 2 copies – total of 6 copies stored by default (highly durable)</strong></li> <li>replication is done immediately and also Free</li> </ul>"
  },
  {
    "id": "18",
    "data": "S3 (Simple Storage Service) <ul> <li>Object based storage on cloud (meaning you cannot install any programs, you use AWS EBS for installing programs in storage)</li> <li><strong>up to 5 terabytes PER one object</strong> <ul> <li>no limit on all objects</li> </ul> </li> <li>files are stored in “buckets”</li> <li>uses universal namespace for buckets, i.e bucket name should be unique throughout whole AWS</li> <li>Uses Rest webservice for API calls</li> <li>100 buckets soft limit (you can request more to AWS)</li> <li>Static web hosting bucket naming convention for URL: <strong><a href='https://.s3-website-.amazonaws.com%C2%A0/' rel='nofollow'>https://.s3-website-.amazonaws.com </a></strong></li> <li>Event notification subscriber: SNS, SES, Lambda</li> </ul> Access <ul> <li>Read after write for <strong>PUTS</strong> (you can immediately access whatever file you upload in s3 buckets)</li> <li>Eventual Consistency for <strong>UPDATE</strong> (overwrite <strong>PUTS</strong>) and <strong>DELETE</strong> (it may take a while before the change propagates throughout different regions)</li> </ul> Storage Details/Capabilities <ul> <li>Key Value mechanism for objects</li> <li>Versioning control (AWS keeps tracks of previous version of object, given that you have enabled versioning for your bucket)</li> <li>Cross Region Replication – buckets can be replicated accross different regions</li> <li>Metadata is also stored (data about your data)</li> <li>Access control can be configured for s3</li> <li><em>Lifecycle Management</em>: frequently accessed files can be stored in standard bucket, moved to infrequent access if not accessed by any user for a few months , and finally moved to glacier if not accessed by any user for a number of years. Lifecycle management allows you to tweak when to move files from certain bucket to another. below are the types of buckets in detail.</li> </ul> Types <ul> <li><strong>S3 Standard</strong> <ul> <li>99.99% availability</li> <li>99.999999999% Durability (eleven 9s)</li> <li>Tiered storage available</li> <li>Lifecycle Management (You can move to archive if files have not been accessed a long time)</li> <li>Versioning</li> <li>Encryption (Encryption at transit: SSL, Encryption at rest: AES-256, different ways of encryption detailed below)</li> <li>Secure Data Access</li> </ul> </li> <li><strong>S3 Infrequent Access</strong> <ul> <li>Lower costs than standard</li> <li>virtually the same except you have only 99.9% availability</li> </ul> </li> <li><strong>Glacier</strong> <ul> <li><strong>40 Terabytes per individual archives</strong> <ul> <li>No limit to all</li> </ul> </li> <li>Archiving option of AWS</li> <li>Extremely low cost</li> <li>same durability</li> <li>takes 3-6 hours before you can access any files</li> </ul> </li> </ul> From Amazon comparison of bucket types: <p class='fix-link-focus'><img src='../../src/img/content/ss25.png' /></p> Additional Option for bucket types:<strong>Reduced Redundancy Storage</strong>: further cheaper version of your buckets,reduces the durabiliy to 99.99% instead of 99.9999999999%, this is for files that can be lost, i.e thumbnails of your pictures that can be regenerated anyways<strong>Important Note: Buckets MUST BE LOWER CAPS</strong><strong>Static website hosting: </strong>buckets can be used to host static website (no javascript, no server side scripting, no processing just plain html files). The benefits of using buckets as static website hosting is for temporary web pages, like for example creating a temporary web page for a movie poster that will be displayed for a few days only. Buckets will then handle your auto-scaling, load-balancing should there be a lot of traffic to visiting your site and you wouldn’t need to worry about your site going down.<strong>Security:</strong>Transit : SSL/TLS At Rest: <ul> <li>Server Side Encryption (SSE)</li> <li>SSE-S3 – S3 Key – AWS S3 has the master key</li> <li><strong>SSE-C – Customer provided Key – Customer is you, ie, you want to have your own keys, not your clients’ keys.</strong></li> <li>Client Side Encryption</li> </ul>"
  },
  {
    "id": "19",
    "data": "Amazon Storage Gateway provides on-premise storage infrastructure for clients. <p class='fix-link-focus'><img src='../../src/img/content/ss26.png'/></p> <strong>Types:</strong> <ul> <li>Gateway Stored Volume – files are stored on site, asynchronously uploads backup to AWS</li> <li>Gateway Cached Volume – frequent accessed files are stored on local, while others are stored in AWS, there is no access to those files if you are connecting via internet and you lose internet connection (best to use Direct Connect in this scenario)</li> <li>Gateway Virtual Tape – for replacing physical tapes, enables you to upload data from virtual tapes to AWS.</li> </ul> <strong>Amazon S3 Import/Export</strong> – enables high speed upload/download of data transfer using Amazon high speed internal network. <strong>Capabilities:</strong> <ul> <li>Import/Export to Disk</li> <li>Import to S3</li> <li>Import to Glacier</li> <li>Export to S3</li> </ul> <strong>Amazon Import/Export Snowball</strong> – Import/Export to S3 via Amazon physical device. It is much faster than specified above, but will require you to purchase/rent Amazon Snowball device. It is usually recommended by Amazon to use this when importing or exporting large amount of data. &nbsp;"
  },
  {
    "id": "20",
    "data": "Amazon Storage Gateway provides on-premise storage infrastructure for clients. <p class='fix-link-focus'><img src='../../src/img/content/ss27.png' /></p> <strong>Types:</strong> <ul> <li>Gateway Stored Volume – files are stored on site, asynchronously uploads backup to AWS</li> <li>Gateway Cached Volume – frequent accessed files are stored on local, while others are stored in AWS, there is no access to those files if you are connecting via internet and you lose internet connection (best to use Direct Connect in this scenario)</li> <li>Gateway Virtual Tape – for replacing physical tapes, enables you to upload data from virtual tapes to AWS.</li> </ul> <strong>Amazon S3 Import/Export</strong> – enables high speed upload/download of data transfer using Amazon high speed internal network. <strong>Capabilities:</strong> <ul> <li>Import/Export to Disk</li> <li>Import to S3</li> <li>Import to Glacier</li> <li>Export to S3</li> </ul> <strong>Amazon Import/Export Snowball</strong> – Import/Export to S3 via Amazon physical device. It is much faster than specified above, but will require you to purchase/rent Amazon Snowball device. It is usually recommended by Amazon to use this when importing or exporting large amount of data. &nbsp;<strong>AWS CloudFront</strong> – is a cloud global content delivery network (CDN) designed to accelerate delivery of your content to users by caching data to an AWS location (called edge location) closer to your user. i.e if your S3 bucket is located in Europe, it would take a lot longer for U.S customers to access these files. CloudFront enables you to create a cache location of your S3 files: <p class='fix-link-focus'><img src='../../src/img/content/ss28.png'/></p> <strong>It is now possible to expedite uploads to S3 by writing directly to an Edge Location.</strong> Updating Cache Data on Cloudfront – While the first 1000 invalidation paths per month are free, additional invalidation paths are $0.005 per request. <strong>CDN Sources:</strong> <ul> <li>S3 Bucket</li> <li>Elastic Load Balancer</li> <li>Route 53</li> <li>EC2</li> <li>Non-AWS resources</li> </ul> <strong>Capabilities/Features</strong> <ul> <li><a href='http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-rtmp.html'>RMTP (Real-Time Messaging Protocol) Distribution </a></li> <li>Web Distribution</li> <li>Configure TTL (Time To Live) of your objects in cloudfront before they are refreshed again</li> <li><a href='http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html'>Restrict Viewer Access </a></li> </ul> <strong>NOTE: CloudFront PCI DSS Compliance – allows processing, storage and transmission of credit card.</strong>"
  },
  {
    "id": "21",
    "data": "<h1>IAM (Identity and Access Management)</h1> Amazon service that enables you to do the following: <ul> <li>Create users</li> <li>Manage users and their access</li> <li>Create Federated User (Temporary Users)</li> <li>Free of charge</li> </ul> IAM User Management <ul> <li>Create, Delete, List Users</li> <li>Manage group memberships, credentials permissions</li> <li><strong>default 100 groups limit, 5000 users limit</strong></li> </ul> Users <ul> <li>in this context, Users are individual people that have access to AWS</li> <li>Users are global and not region specific</li> </ul> Groups <ul> <li>Collection of users</li> <li>Users can belong to multiple groups (<strong>10 default limit</strong>)</li> <li>Groups cannot be nested, i.e Groups cannot be assigned to other groups</li> </ul> Roles <ul> <li>“An IAM <span class='emphasis'><em>role</em></span> is similar to a user, in that it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person”</li> <li><strong>250 default roles limit</strong></li> <li>Roles can be assigned to other AWS accounts <ul> <li>Power User role – all access except group management</li> <li>Administrator Access role – All account resources except AWS account info</li> </ul> </li> </ul> <strong>IAM Owner</strong>– is the one who created the AWS account Policies <ul> <li>JSON format rules that define access</li> <li>Policies can be attached to roles/groups/users</li> </ul> <strong>Policy Simulator</strong> – enables you to test out your policies, AWS provided service free of charge. http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html <em><strong>Multifactor Authentication </strong></em><strong> – </strong>additional layer of authentication other than just password, this can be any third party device, virtual authenticator, STS (Security Token Service), SMS authentication. <ul> <li>When user is trying to login, a code will be sent to his MFA device, he then needs to input the code after providing his password, this ensures another layer of security, and your access is not compromised easily should someone finds out about your password.</li> <li>This can be enforced in API calls for developers when calling sensitive API calls to AWS.</li> </ul> <em><strong>Identity Federation</strong></em> – allows third party accounts to login to your AWS, i.e using LDAP, facebook, google, etc, no need to create AWS IAM user <div><strong>NOTE:</strong> Active Directory authentication is possible in AWS through SAML, authentication is done first in AD before being passed to AWS</div> <div></div> <em><strong>BEST PRACTICES: </strong></em> <ul> <li><em> </em><em>Root/privileged users should have MFA</em></li> <li>Grant only least access privileges</li> <li>Each users should have individual IAMs (not sharing accounts)</li> <li>Use Groups for managing users</li> <li>enforce a strong password policy</li> <li>assign IAM roles for your applications</li> <li>Rotate credentials</li> <li>Track what users are doing with cloudtrail (audit log service for AWS)</li> </ul>"
  },
  {
    "id": "22",
    "data": "<h1>IAM (Identity and Access Management)</h1> Amazon service that enables you to do the following: <ul> <li>Create users</li> <li>Manage users and their access</li> <li>Create Federated User (Temporary Users)</li> <li>Free of charge</li> </ul> IAM User Management <ul> <li>Create, Delete, List Users</li> <li>Manage group memberships, credentials permissions</li> <li><strong>default 100 groups limit, 5000 users limit</strong></li> </ul> Users <ul> <li>in this context, Users are individual people that have access to AWS</li> <li>Users are global and not region specific</li> </ul> Groups <ul> <li>Collection of users</li> <li>Users can belong to multiple groups (<strong>10 default limit</strong>)</li> <li>Groups cannot be nested, i.e Groups cannot be assigned to other groups</li> </ul> Roles <ul> <li>“An IAM <span class='emphasis'><em>role</em></span> is similar to a user, in that it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person”</li> <li><strong>250 default roles limit</strong></li> <li>Roles can be assigned to other AWS accounts <ul> <li>Power User role – all access except group management</li> <li>Administrator Access role – All account resources except AWS account info</li> </ul> </li> </ul> <strong>IAM Owner</strong>– is the one who created the AWS account Policies <ul> <li>JSON format rules that define access</li> <li>Policies can be attached to roles/groups/users</li> </ul> <strong>Policy Simulator</strong> – enables you to test out your policies, AWS provided service free of charge. http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html <em><strong>Multifactor Authentication </strong></em><strong> – </strong>additional layer of authentication other than just password, this can be any third party device, virtual authenticator, STS (Security Token Service), SMS authentication. <ul> <li>When user is trying to login, a code will be sent to his MFA device, he then needs to input the code after providing his password, this ensures another layer of security, and your access is not compromised easily should someone finds out about your password.</li> <li>This can be enforced in API calls for developers when calling sensitive API calls to AWS.</li> </ul> <em><strong>Identity Federation</strong></em> – allows third party accounts to login to your AWS, i.e using LDAP, facebook, google, etc, no need to create AWS IAM user <div><strong>NOTE:</strong> Active Directory authentication is possible in AWS through SAML, authentication is done first in AD before being passed to AWS</div> <div></div> <em><strong>BEST PRACTICES: </strong></em> <ul> <li><em> </em><em>Root/privileged users should have MFA</em></li> <li>Grant only least access privileges</li> <li>Each users should have individual IAMs (not sharing accounts)</li> <li>Use Groups for managing users</li> <li>enforce a strong password policy</li> <li>assign IAM roles for your applications</li> <li>Rotate credentials</li> <li>Track what users are doing with cloudtrail (audit log service for AWS)</li> </ul>Credits to <a href='http://aclo/' rel='nofollow'>http://aclo</a> What is <strong>VPC Peering</strong>? <ul> <li> Connection between two VPCs (single or with other AWS Account) within a single region.</li> <li>This is done via private IP address.</li> <li>Technology used is existing infrastructure of VPC, it is neither a gateway or a VPN connection.</li> </ul> Example VPC Peering: <p class='fix-link-focus'><img src='../../src/img/content/ss29.png'/></p> <strong>Transitive Peering NOT Supported</strong> – VPC A cannot access VPC C via VPC B <p class='fix-link-focus'><img src ='../../src/img/content/ss30.png'/></p> If VPC B is change to this cidr block as above, it breaks the connection as there is overlapping internal address range (CIDR block). <strong>Limitations:</strong> <ul> <li>no overlapping CIDR blocks</li> <li>no peering connections</li> <li>cannot be on different regions</li> </ul>"
  },
  {
    "id": "23",
    "data": "<strong>VPC</strong> – Virtual Private Cloud, is your network configuration for your AWS resources <ul> <li>public subnet by default means you have one subnet in your route tables whose target is a Internet Gateway enabling access to the public</li> <li>on there other hand private subnet for you private resources that shouldn’t be available to public</li> </ul> <strong>VPC Wizard:</strong> <ul> <li>can opt for vpc with public/private subnet/ and with Hardware VPN accese</li> </ul> <strong>VPC Security Groups</strong> <ul> <li>Configuration of inbound and outbound traffic for your resources</li> <li>All outbound traffic by default is allowed when you create a new one</li> <li>Security groups is stateful – if you enable an inbound traffic, traffic will also flow outbound regardless of the security group</li> <li>You can specify allow rules, but not deny rules.</li> </ul> <strong>VPC Network ACL (Access Control List)</strong> <ul> <li>By default allows all inbound and outbound</li> <li>ACL is stateless – contrast to Security group</li> <li>Associated to subnet</li> <li>Evaluated per order</li> </ul> <strong>Route Table</strong> A routing table is a set of rules, often viewed in table format, that is used to determine where data <a href='http://searchnetworking.techtarget.com/definition/packet'>packet</a>s traveling over an <a href='http://searchunifiedcommunications.techtarget.com/definition/Internet-Protocol'>Internet Protocol</a> (IP) network will be directed. All IP-enabled devices, including <a href='http://searchnetworking.techtarget.com/definition/router'>router</a>s and <a href='http://searchtelecom.techtarget.com/definition/switch'>switch</a>es, use routing tables. <p class='fix-link-focus'><img class='alignnone size-full wp-image-983' src='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss9.png?w=700' sizes='(max-width: 559px) 100vw, 559px' srcset='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss9.png 559w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss9.png?w=150 150w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss9.png?w=300 300w' alt='ss.PNG' data-attachment-id='983' data-permalink='https://sysdotoutdotprint.wordpress.com/2017/02/10/amazon-web-services-vpc-quick-exam-notes-solutions-architect-associate/ss-8/' data-orig-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss9.png?w=700' data-orig-size='559,500' data-comments-opened='1' data-image-meta='{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}' data-image-title='ss' data-image-description='' data-medium-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss9.png?w=700?w=300' data-large-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss9.png?w=700?w=559' /></p> <strong>NETWORK ACL vs Security Groups</strong> Security groups filters which traffic goes in and out to your instances, ie which ports can be access, in contrast Network ACL operates at a subnet level, ie which IP can SSH to your instance <ul> <li>Network ACL – applies to network level, that it can apply to many instances</li> <li>Security Group – applies only to instance level, so if you need it in one instance only may need to apply to Security group only</li> </ul> <strong>VPN Connections</strong> <ul> <li><strong>By default, instances that you launch into a virtual private cloud (VPC) can’t communicate with your own network. You can enable access to your network from your VPC by attaching a virtual private gateway to the VPC</strong></li> <li>AWS Hardware VPN – enables customers to connect between VPC and remote network</li> <li>AWS Direct Connect – provides private connection to AWS without internet</li> <li>CloudHub – more than one remote network</li> <li>Software VPN – can be setup via software VPN</li> </ul> <strong>Components</strong> <ul> <li>Virtual Private Gateway – VPN connector on AWS side</li> <li>Customer Gateway – customer side</li> </ul> <p class='fix-link-focus'><img class='alignnone size-full wp-image-966' src='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss7.png?w=700' sizes='(max-width: 616px) 100vw, 616px' srcset='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss7.png 616w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss7.png?w=120 120w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss7.png?w=241 241w' alt='ss.PNG' data-attachment-id='966' data-permalink='https://sysdotoutdotprint.wordpress.com/2017/02/10/amazon-web-services-vpc-quick-exam-notes-solutions-architect-associate/ss-6/' data-orig-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss7.png?w=700' data-orig-size='616,767' data-comments-opened='1' data-image-meta='{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}' data-image-title='ss' data-image-description='' data-medium-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss7.png?w=700?w=241' data-large-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss7.png?w=700?w=616' /></p> Redundancy: <p class='fix-link-focus'><img class='alignnone size-full wp-image-967' src='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss8.png?w=700' sizes='(max-width: 365px) 100vw, 365px' srcset='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss8.png 365w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss8.png?w=89 89w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss8.png?w=178 178w' alt='ss.PNG' data-attachment-id='967' data-permalink='https://sysdotoutdotprint.wordpress.com/2017/02/10/amazon-web-services-vpc-quick-exam-notes-solutions-architect-associate/ss-7/' data-orig-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss8.png?w=700' data-orig-size='365,614' data-comments-opened='1' data-image-meta='{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}' data-image-title='ss' data-image-description='' data-medium-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss8.png?w=700?w=178' data-large-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss8.png?w=700?w=365' /></p> <ul> <li>Route Tables – determine where traffic is determined</li> </ul> <strong>NAT – Network Address Translation</strong> <ul> <li>Enables private instances to connect to the internet, but prevents the internet from initiating connection with instances</li> </ul> How it works: <p class='fix-link-focus'><img class='alignnone size-full wp-image-500' src='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss16.png?w=700' sizes='(max-width: 700px) 100vw, 700px' srcset='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss16.png?w=700 700w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss16.png?w=1400 1400w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss16.png?w=150 150w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss16.png?w=300 300w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss16.png?w=768 768w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss16.png?w=1024 1024w' alt='ss1' data-attachment-id='500' data-permalink='https://sysdotoutdotprint.wordpress.com/2017/02/10/amazon-web-services-vpc-quick-exam-notes-solutions-architect-associate/ss1-7/' data-orig-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss16.png?w=700' data-orig-size='1785,1032' data-comments-opened='1' data-image-meta='{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}' data-image-title='ss1' data-image-description='' data-medium-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss16.png?w=700?w=300' data-large-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss16.png?w=700?w=700' /> private IP is replaced with NATs public IP</p> <strong>NAT Key points</strong> <ul> <li> needs to be launched in public subnet</li> <li>needs to be assocaited with public/elastic ip address</li> <li>disable source/destination flag check – this flag directly conflics with how NAT works as per above</li> <li>Security group should allow inbound/outbound</li> <li>Route table should configured to have an internet gateway</li> </ul> <strong>NAT Gateway vs NAT Instance</strong> <table> <tbody> <tr> <td>NAT Gateway</td> <td>NAT Instance</td> </tr> <tr> <td>AWS managed instance</td> <td>User created instance, configured to be NAT</td> </tr> <tr> <td>10 gbps burst</td> <td>availability and bandwith depends on the instance type</td> </tr> <tr> <td>no Security Group</td> <td>must have security group</td> </tr> <tr> <td>one elastice IP address associated</td> <td>manually disable source/destination check</td> </tr> <tr> <td> <ul> <li>specific AZ, with redundancy</li> <li>TCP, UDP, ICMP support</li> <li>ports – 1024- 65535</li> <li>cannot send through VPC endpoints</li> </ul> </td> <td></td> </tr> </tbody> </table> <strong>High Availabiltiy NAT Instance design:</strong> <ul> <li>one NAT instance per AZ</li> <li>all private subnet route teables to the same zone NAT instance</li> <li>configure AutoScaling for instances</li> <li>have it grow if CPU reaches a certain threshold</li> <li>create bootstrap scripts for updating NAT instances</li> </ul> <strong>BASTION</strong> <ul> <li>structure for fortication to protect things behind it</li> <li>in AWS also known as a Jump Server</li> <li>used to access instances in private subnet</li> </ul> <strong>How it works:</strong> There is no direct access available to connect to your Web Server, you would have to SSH to bastion instance first, users access your server through load balancer. <p class='fix-link-focus'><img class='alignnone size-full wp-image-522' src='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss21.png?w=700' sizes='(max-width: 700px) 100vw, 700px' srcset='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss21.png?w=700 700w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss21.png?w=1400 1400w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss21.png?w=150 150w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss21.png?w=300 300w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss21.png?w=768 768w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss21.png?w=1024 1024w' alt='ss2' data-attachment-id='522' data-permalink='https://sysdotoutdotprint.wordpress.com/2017/02/10/amazon-web-services-vpc-quick-exam-notes-solutions-architect-associate/ss2/' data-orig-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss21.png?w=700' data-orig-size='1785,1032' data-comments-opened='1' data-image-meta='{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}' data-image-title='ss2' data-image-description='' data-medium-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss21.png?w=700?w=300' data-large-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss21.png?w=700?w=700' /></p>"
  },
  {
    "id": "24",
    "data": "<h1>Route 53 – AWS DNS Server</h1> <strong>Supports the following</strong> <ul> <li>NS (name server)</li> <li>CNAME (canonical name record)</li> <li>SRV (service locator)</li> </ul> <strong>Terms</strong> <ul> <li>Domain name – URL for website</li> <li>Domain Registrar – company accreditd by ICANN to process domain registration</li> <li>Domain Registry – company owns right to sell domains <ul> <li><strong>Route 53 can act as a domain Registry top register domains</strong></li> </ul> </li> <li>Domain Reseller – company sessl domain names for registrars</li> <li>Top-level Domain – the last part of domain name (.com, .org) <ul> <li>generic top level – such as .com, .hockey, .bike</li> <li>geographic – associated with countries (.ph, .au)</li> </ul> </li> </ul> <strong>Domain Name System Terms</strong> <ul> <li>Alias resource record set – record set you can create with route 53 to route traffic to AWS resource</li> <li>Authoritative name server – Name server that response to request from a DNS resolver</li> <li>DNS Query – query submitted by devices to DNS Server</li> <li>DNS resolver – DNS server managed by internet service provider</li> <li>Domain Name System – DNS, worldwide network of servers that enable translation of URLs to IP addresses</li> <li>Hosted Zone – container for resource record sets (contains how to route trafic for domain and subdomain</li> <li>IP Address- number assigned to device on the internet (laptop, phones etc) <ul> <li>IPv4</li> <li>IPv6</li> </ul> </li> <li>Name servers – Servers in DNS</li> <li>private DNS – local version of DNS</li> <li>TTL – Time to Live</li> </ul><h1>Route 53 – AWS DNS Server</h1> <strong>Supports the following</strong> <ul> <li>NS (name server)</li> <li>CNAME (canonical name record)</li> <li>SRV (service locator)</li> </ul> <strong>Terms</strong> <ul> <li>Domain name – URL for website</li> <li>Domain Registrar – company accreditd by ICANN to process domain registration</li> <li>Domain Registry – company owns right to sell domains <ul> <li><strong>Route 53 can act as a domain Registry top register domains</strong></li> </ul> </li> <li>Domain Reseller – company sessl domain names for registrars</li> <li>Top-level Domain – the last part of domain name (.com, .org) <ul> <li>generic top level – such as .com, .hockey, .bike</li> <li>geographic – associated with countries (.ph, .au)</li> </ul> </li> </ul> <strong>Domain Name System Terms</strong> <ul> <li>Alias resource record set – record set you can create with route 53 to route traffic to AWS resource</li> <li>Authoritative name server – Name server that response to request from a DNS resolver</li> <li>DNS Query – query submitted by devices to DNS Server</li> <li>DNS resolver – DNS server managed by internet service provider</li> <li>Domain Name System – DNS, worldwide network of servers that enable translation of URLs to IP addresses</li> <li>Hosted Zone – container for resource record sets (contains how to route trafic for domain and subdomain</li> <li>IP Address- number assigned to device on the internet (laptop, phones etc) <ul> <li>IPv4</li> <li>IPv6</li> </ul> </li> <li>Name servers – Servers in DNS</li> <li>private DNS – local version of DNS</li> <li>TTL – Time to Live</li> </ul>"
  },
  {
    "id": "25",
    "data": "SQS – Simple Queue Service <strong>Message Size</strong> – 256kb <strong>Message Attributes:</strong> <ul> <li>Name</li> <li>Type (String/Binary/Number)</li> <li>Value – user value</li> </ul> <strong>URL</strong> : The following is the queue URL for a queue named <code class='code'>MyQueue</code> owned by a user with the AWS account number <code class='code'>123456789012</code>. <pre class='programlisting'><code class='nohighlight'>http://sqs.us-east-2.amazonaws.com/123456789012/MyQueue</code></pre> <strong>Key Terms:</strong> <ul> <li><strong>Receipt Handle</strong> – each time you recieve a message you get a different receipt handle, this is used to handle when you request to delete a message</li> <li><strong>Message Deduplication Id</strong> – the token used for deduplication of sent messages, any messages sent with same aren’t delivered during 5 minute deduplication interval</li> <li><strong>Message Group Id</strong> – group identifier</li> <li><strong>Sequence number</strong></li> <li><strong>Inflight Message – </strong>message received but not yet deleted</li> <li><strong>Dead Letter Queue – </strong>Queue for others queues to target for messages that can’t be processed, this allow messages to be isolated and help determine why it wasn’t procesed.</li> </ul> <strong>Two Types of Queues:</strong> <ul> <li>Standard <ul> <li>high throughput</li> <li>guaranteed at least once delivery (can be more than once)</li> </ul> </li> <li>FIFO <ul> <li>ordered messages</li> <li>first in first out delivery</li> </ul> </li> </ul> <strong>Visibility Timeout (default 30 seconds, Maximum: 12 hours.)</strong> <strong> </strong>When a consumer receives and processes a message from a queue, it remains in the queue AWS does not automatically delete the message, consumer must request a delete. This is because queue service are in a distributed system there is no guarantee the message has been received already (ie connection broken, component fail etc) A message once received, to prevent other consumer to reprocess the message, there is a visibility timeout, which a message is not visible to other components. (not a guarantee for standard queues) <strong>Increasing the visibility timeout will decrease cost over time.</strong> You can turn off visibility timeout How it works: <p class='fix-link-focus'><img class='alignnone size-full wp-image-724' src='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss.png?w=700' sizes='(max-width: 592px) 100vw, 592px' srcset='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss.png 592w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss.png?w=150 150w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss.png?w=300 300w' alt='ss' data-attachment-id='724' data-permalink='https://sysdotoutdotprint.wordpress.com/2017/02/22/amazon-web-services-sqs-quick-notes-solutions-architect-associate/ss/' data-orig-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss.png?w=700' data-orig-size='592,539' data-comments-opened='1' data-image-meta='{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}' data-image-title='ss' data-image-description='' data-medium-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss.png?w=700?w=300' data-large-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss.png?w=700?w=592' /></p> <strong>Delay Queues</strong> Works similar like visibility timeout, except message is invisible when first added to queue: <p class='fix-link-focus'><img class='alignnone size-full wp-image-737' src='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss3.png?w=700' sizes='(max-width: 598px) 100vw, 598px' srcset='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss3.png 598w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss3.png?w=150 150w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss3.png?w=300 300w' alt='ss' data-attachment-id='737' data-permalink='https://sysdotoutdotprint.wordpress.com/2017/02/22/amazon-web-services-sqs-quick-notes-solutions-architect-associate/ss-2/' data-orig-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss3.png?w=700' data-orig-size='598,187' data-comments-opened='1' data-image-meta='{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}' data-image-title='ss' data-image-description='' data-medium-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss3.png?w=700?w=300' data-large-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss3.png?w=700?w=598' /></p> <strong>Long Polling</strong> Long polling reduces number of empty responses by allowing SQS to wait until a message is available before sending a response. it returns a message as soon as any messages becomes available. By default SQS uses short polling, querying only a subset of the servers to determine whether any messages are available. Short Polling occurs when WaitTimeSeconds parameter of a message is set to 0. <strong>Short polling may fail to retrieve messages sometimes, but if no messages can be retrieved after multiple attempts, permissions are the more likely cause.</strong> <strong>Billing</strong> 1 million messages free per month under free tier"
  },
  {
    "id": "26",
    "data": "<strong>SNS</strong> – Simple Notification Service, allows delivery or sending of message to subscribing endpoints. <strong>Two Types of clients:</strong> <ul> <li>Publisher – publishes the message to a topic</li> <li>Subscriber/Consumer – consumes the message <ul> <li>Lambda</li> <li>SQS</li> <li>HTTP/S</li> <li>Email</li> <li>SMS</li> </ul> </li> </ul> <strong>Publisher -&gt; Amazon SNS Topic -&gt; Subscriber</strong> <strong>Common scenario</strong>: Fanout Publisher receives for example an order for some product, it is sent to Topic, multiple SQS queues consumes the message and is allocated to seperate instances for parallel processing <p class='fix-link-focus'><img class='alignnone size-full wp-image-800' src='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss4.png?w=700' sizes='(max-width: 389px) 100vw, 389px' srcset='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss4.png 389w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss4.png?w=150 150w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss4.png?w=300 300w' alt='ss.PNG' data-attachment-id='800' data-permalink='https://sysdotoutdotprint.wordpress.com/2017/02/24/amazon-web-service-simple-notification-service/ss-3/' data-orig-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss4.png?w=700' data-orig-size='389,151' data-comments-opened='1' data-image-meta='{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}' data-image-title='ss' data-image-description='' data-medium-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss4.png?w=700?w=300' data-large-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss4.png?w=700?w=389' /></p> <strong>Other uses:</strong> <ul> <li>Application Alerts</li> <li>Push Email/Text Messaging</li> <li>Mobile Push Notification</li> </ul> &nbsp;"
  },
  {
    "id": "27",
    "data": "<h1><strong>Kinesis</strong> enables to collect and process streams of data records in real time.</h1> <strong>What you can you do:</strong> <ul> <li>accelerated log and data feed intake and processing -ie large amount of applicaiton logs, market data feeds, web clickstream data, social media</li> <li>real-time metrics and reporting</li> <li>real-time data analytics</li> <li>Complex stream processing</li> </ul> <strong>High level architecture</strong> <p class='fix-link-focus'><img class='alignnone size-full wp-image-812' src='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss5.png?w=700' sizes='(max-width: 606px) 100vw, 606px' srcset='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss5.png 606w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss5.png?w=150 150w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss5.png?w=300 300w' alt='ss' data-attachment-id='812' data-permalink='https://sysdotoutdotprint.wordpress.com/2017/02/24/amazon-webservices-kinesis/ss-4/' data-orig-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss5.png?w=700' data-orig-size='606,302' data-comments-opened='1' data-image-meta='{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}' data-image-title='ss' data-image-description='' data-medium-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss5.png?w=700?w=300' data-large-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss5.png?w=700?w=606' /></p> <strong>Terminologies</strong>: <ul> <li><strong>Stream</strong> – ordered sequence of data</li> <li><strong>Data records</strong> – unit of data stored in kinesis (contains sequence number, partition key, data blob) up to <strong>1mb</strong></li> <li><strong>Retention period –</strong> configurable in hourly increments from 24 to 168 hours (1 to 7 days), default <strong>24 hrs</strong></li> <li><strong>Producers</strong> – those who create the stream</li> <li><strong>Consumers</strong> – stream consumers</li> <li><strong>Stream</strong> application</li> <li><strong>Shard</strong> – group of data records in a stream</li> <li><strong>partition</strong> key – segregates data records</li> <li>sequence number</li> </ul>"
  },
  {
    "id": "28",
    "data": "<h1>Cloudwatch – enables you to monitor AWS resources and applications you run in real time, Sends notification</h1> <strong>How it works:</strong> <p class='fix-link-focus'><img class=' size-full wp-image-864 alignnone' src='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss6.png?w=700' sizes='(max-width: 555px) 100vw, 555px' srcset='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss6.png 555w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss6.png?w=150 150w, https://sysdotoutdotprint.files.wordpress.com/2017/02/ss6.png?w=300 300w' alt='ss' data-attachment-id='864' data-permalink='https://sysdotoutdotprint.wordpress.com/2017/02/24/amazon-webservices-cloudwatch/ss-5/' data-orig-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss6.png?w=700' data-orig-size='555,381' data-comments-opened='1' data-image-meta='{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}' data-image-title='ss' data-image-description='' data-medium-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss6.png?w=700?w=300' data-large-file='https://sysdotoutdotprint.files.wordpress.com/2017/02/ss6.png?w=700?w=555' /></p> Concepts: <ul> <li>Namespaces- container of cloudwatch</li> <li>Metrics – represents time ordered set of data points published to cloudwatch <ul> <li><strong>Metrics can’t be deleted, automatically expire after 15 months </strong></li> <li>Time stamps</li> <li>Metrics retention <ul> <li>period 60 seconds – available for 15 days</li> <li>period of 300 seconds – 63 days</li> <li>period of 3600 seconds – 455 days</li> <li>Amazon CloudWatch stores metrics for terminated Amazon EC2 instances or deleted Elastic Load Balancers for 2 weeks.</li> </ul> </li> </ul> </li> <li>Dimension – key value that identifies metric <ul> <li>Comibination of dimension are considered seperate metric</li> </ul> </li> <li>Statistics <ul> <li>Minimum</li> <li>Maximum</li> <li>Sum</li> <li>Average</li> <li>Sample count – count of data points</li> <li>pNN.NN – value of specified percentile</li> </ul> </li> <li>Units – unit of measure – Bytes, seconds, counts, percent</li> </ul> <strong>Common Available metrics:</strong> <ul> <li>RDS <ul> <li>CPU</li> <li>Connections</li> <li>Memory</li> <li>Read/Write Throughput/latency</li> </ul> </li> <li>EC2 <ul> <li>CPU</li> <li>Disk</li> <li>Network</li> <li>Autoscaling size, instances</li> </ul> </li> <li>S3 <ul> <li>Bucket size</li> <li>number of objects</li> <li>requests/put/delete/get</li> </ul> </li> <li>SQS <ul> <li>Number of message</li> <li>message size</li> <li>delayed messages</li> </ul> </li> </ul> <strong>Memory Utilization requires a custom Cloudwatch metric</strong> CloudWatch <strong>CANNOT</strong> see the following: <ul> <li>web server visible metrics such as number failed transaction requests -Too detailed for EC2 – Amazon don’t even want to know whether you have or haven’t even installed a web server.</li> <li>operating system visible metrics such as memory utilization – Too detailed for EC2 – Amazon don’t want to interact with your operating system.</li> </ul> Limits: <table id='d0e904' border='0' cellspacing='0'> <tbody> <tr> <td>Actions</td> <td>5/alarm. This limit cannot be changed.</td> </tr> <tr> <td>Alarms</td> <td>10/month/customer for free. 5000 per region per account.</td> </tr> </tbody> </table>"
  },
  {
    "id": "29",
    "data": "<strong>Amazon Kinesis –</strong> real time processing of streaming data at massive scale ie website clickstream, application logs, social media feeds <strong>Uses:</strong> <ul> <li>Used to consume big data – can analyze from big amount of data, for example twitter can scan all the tweets for negative/positive comments</li> <li>Processing large amount of data</li> </ul> <strong>Exam tips</strong> <ul> <li>Business Intelligence – think Redshift</li> <li>Consuming big data – think Kineses</li> <li>Big Data Processing – think Elastic Map Reduce</li> </ul> <strong>EC2 – EBS Backed vs Instance Store</strong> <ul> <li>EBS back volumes are persistent, Instance store are not persistent (Ephemeral)</li> <li>EBS can be dettached/reattached</li> <li>Instance store cannot be detached</li> <li>EBS volume can be stopped, data will persist</li> <li>Instance store cannot be stopped, if an EC2 has EBS root volume is stopped with Instance stores as additional volumes, all data with instance stores will be lost</li> </ul> <strong>Exam Tips</strong> <ul> <li>EBS Backed – Store Data Long Term</li> <li>Instance Store – should be used for long term</li> </ul> <strong>OpsWorks</strong> <ul> <li>Orchestration Service that uses Chef</li> <li>Chef – converts infra to code, to maintain consistent state, consists of <strong>recipes, cook books, chef</strong></li> </ul> <strong>Elastic Transcode</strong> <ul> <li>Convert media to different formats, ie one video can be played to mp4, to iphone or ipad, laptop etc</li> <li>pay based on minutes per transcode, and resolution</li> </ul> <strong>SWF – Actors</strong> <ul> <li>Workflow starter</li> <li>Deciders</li> <li>Activity Worker</li> <li>Domain – group of related workflows</li> </ul> <strong>EC2 – get public IP address</strong> <ul> <li>curl http://169.254.169,254/latest/meta-data/</li> <li>META DATA for public ip, USER DATA is for user data</li> </ul> <strong>Consolidated Billing</strong> <ul> <li>Paying Account – linked to different accounts (dev/production/backoffice)</li> <li>paying account will get all the monthly bill, does not access the resources of different accounts</li> <li>Soft limit is 20</li> <li>Benefits: <ul> <li>One bill per AWS account,</li> <li>easy to track all the costs,</li> <li>Volume pricing discount for all accounts (AWS gives you discount the more you use their services, for example for S3 since there are multiple users, you use more gb, and hence take advantage of the discount provided, instead of paying individual)</li> </ul> </li> </ul> <strong>Tagging and Resource Group</strong> <ul> <li>Tags- are key value pair for metadata, ie. EC2 can have tags</li> <li>Resource Group make it easy for you to group resources using your tags, ie group your resources based on region or by dev or whatever metadata you specify</li> <li>you can export as csv for resource group so you can generate reports</li> <li><strong>Tag Editor</strong> – enables you to edit all your resource and add them tags</li> </ul> &nbsp; <strong>Amazon Lex is a service for building conversational interfaces using voice and text. Polly is a service that turns text into lifelike speech.</strong><strong>Amazon Kinesis –</strong> real time processing of streaming data at massive scale ie website clickstream, application logs, social media feeds <strong>Uses:</strong> <ul> <li>Used to consume big data – can analyze from big amount of data, for example twitter can scan all the tweets for negative/positive comments</li> <li>Processing large amount of data</li> </ul> <strong>Exam tips</strong> <ul> <li>Business Intelligence – think Redshift</li> <li>Consuming big data – think Kineses</li> <li>Big Data Processing – think Elastic Map Reduce</li> </ul> <strong>EC2 – EBS Backed vs Instance Store</strong> <ul> <li>EBS back volumes are persistent, Instance store are not persistent (Ephemeral)</li> <li>EBS can be dettached/reattached</li> <li>Instance store cannot be detached</li> <li>EBS volume can be stopped, data will persist</li> <li>Instance store cannot be stopped, if an EC2 has EBS root volume is stopped with Instance stores as additional volumes, all data with instance stores will be lost</li> </ul> <strong>Exam Tips</strong> <ul> <li>EBS Backed – Store Data Long Term</li> <li>Instance Store – should be used for long term</li> </ul> <strong>OpsWorks</strong> <ul> <li>Orchestration Service that uses Chef</li> <li>Chef – converts infra to code, to maintain consistent state, consists of <strong>recipes, cook books, chef</strong></li> </ul> <strong>Elastic Transcode</strong> <ul> <li>Convert media to different formats, ie one video can be played to mp4, to iphone or ipad, laptop etc</li> <li>pay based on minutes per transcode, and resolution</li> </ul> <strong>SWF – Actors</strong> <ul> <li>Workflow starter</li> <li>Deciders</li> <li>Activity Worker</li> <li>Domain – group of related workflows</li> </ul> <strong>EC2 – get public IP address</strong> <ul> <li>curl http://169.254.169,254/latest/meta-data/</li> <li>META DATA for public ip, USER DATA is for user data</li> </ul> <strong>Consolidated Billing</strong> <ul> <li>Paying Account – linked to different accounts (dev/production/backoffice)</li> <li>paying account will get all the monthly bill, does not access the resources of different accounts</li> <li>Soft limit is 20</li> <li>Benefits: <ul> <li>One bill per AWS account,</li> <li>easy to track all the costs,</li> <li>Volume pricing discount for all accounts (AWS gives you discount the more you use their services, for example for S3 since there are multiple users, you use more gb, and hence take advantage of the discount provided, instead of paying individual)</li> </ul> </li> </ul> <strong>Tagging and Resource Group</strong> <ul> <li>Tags- are key value pair for metadata, ie. EC2 can have tags</li> <li>Resource Group make it easy for you to group resources using your tags, ie group your resources based on region or by dev or whatever metadata you specify</li> <li>you can export as csv for resource group so you can generate reports</li> <li><strong>Tag Editor</strong> – enables you to edit all your resource and add them tags</li> </ul> &nbsp; <strong>Amazon Lex is a service for building conversational interfaces using voice and text. Polly is a service that turns text into lifelike speech.</strong>"
  },
  {
    "id": "30",
    "data": "<strong>CloudTrail</strong> <ul> <li>You can use AWS CloudTrail to get a history of AWS API calls and related events for your account. This includes calls made by using the AWS Management Console, AWS SDKs, command line tools, and higher-level AWS services.</li> </ul> <strong>Cloudfront</strong> <ul> <li>Amazon CloudFront can handle data transfer rate 1,000 Mbps and 1000 requests per second.</li> </ul> <strong>S3</strong> <ul> <li>S3 Standard – IA offers the high durability, throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee.</li> <li>Only difference of IA with standard is 99.99 availibility!</li> <li>IA has minimum of 128KB bytes, S3 standard has 0 bytes minimum</li> <li>S3 does support website redirects.</li> <li>Using IPv6 support for Amazon S3, applications can connect to Amazon S3 without needing any IPv6 to IPv4 translation software or systems.</li> <li>Using an encryption client library, such as the Amazon S3 Encryption Client, you retain control of the keys and complete the encryption and decryption of objects client-side using an encryption library of your choice. Some customers prefer full end-to-end control of the encryption and decryption of objects; that way, only encrypted objects are transmitted over the Internet to Amazon S3.</li> <li>CRR replicates every object-level upload that you directly make to your source bucket. The metadata and ACLs associated with the object are also part of the replication.</li> </ul> <strong>Glacier</strong> <ul> <li>Because Amazon S3 maintains the mapping between your user-defined object name and Amazon Glacier’s system-defined identifier, Amazon S3 objects that are stored using the Amazon Glacier option are only accessible through the Amazon S3 APIs or the Amazon S3 Management Console.</li> </ul> <strong>ELB</strong> <ul> <li>ELB supports ipv6</li> <li><strong>Elastic Load Balancing</strong> offers<strong> two types of load balancers</strong> that both feature high availability, automatic scaling, and robust security.</li> <li>These include the <a href='https://aws.amazon.com/elasticloadbalancing/classicloadbalancer/'>Classic Load Balancer</a> that routes traffic based on either application or network level information, and the <a href='https://aws.amazon.com/elasticloadbalancing/applicationloadbalancer/'>Application Load Balancer</a> that routes traffic based on advanced application level information that includes the content of the request.</li> <li>The Classic Load Balancer is ideal for simple load balancing of traffic across multiple EC2 instances, while the Application Load Balancer is ideal for applications needing advanced routing capabilities, microservices, and container-based architectures.</li> <li>Two components: <ul> <li>the load balancers</li> <li>controller service – verify the load balancers</li> </ul> </li> <li>To ensure traffic is evenly distributed: <strong>“Enable Cross-Zone Load Balancing”</strong></li> <li>Connection draining is the concept of ensuring traffic are not sent anymore to instances that are deregistering or unhealthy.</li> </ul> <strong>EC2</strong> <ul> <li>Cluster group can only be in one AZ</li> <li>Amazon’s SLA guarantees a Monthly Uptime Percentage of at least 99.95% for Amazon EC2 and Amazon EBS within a Region.</li> <li>EBS volumes can be attached to an ec2 instance in the same AZ</li> <li>The AMIs will need to be copied to the new Region prior to deployment.</li> </ul> <strong>RDS</strong> <ul> <li>By default, the scan operation processes data sequentially. DynamoDB returns data to the application in 1 MB increments, and an application performs additional scan operations to retrieve the next 1 MB of data.</li> <li>The easiest way would be to take a snapshot of your DB Instance outside VPC and restore it to VPC by specifying the DB Subnet Group you want to use.</li> <li>To automatically failover from one geographic location to another you should use Multi-AZ for RDS.</li> <li>You should implement database partitioning and spread your data across multiple DB Instances.</li> <li>Databases generally do not require public access from the Internet, so a private subnet is the better choice from a security perspective. /28 is the smallest possible subnet in an AWS VPC.</li> <li><em><strong>RDS replication : MULTI-AZ – Synchronous , Read-Replica – Asynch</strong></em></li> <li>At this time, you cannot have a multi-AZ copy of your read replica.</li> <li>Read Replicas are supported by Amazon RDS for MySQL and PostgreSQL.</li> <li>Infrequent IO:Amazon RDS Magnetic Storage would be the most suitable.</li> <li>At the present time, encrypting an existing DB Instance is not supported. To use Amazon RDS encryption for an existing database, create a new DB Instance with encryption enabled and migrate your data into it.</li> </ul> <strong>SMS (Server Migration Service) </strong> <ul> <li>Improvement of VM Import/Export</li> <li>Simplify migration process, orchetrate multi-server migrations, test, support, minimize downtime</li> <li>50 concurrent VM migrations per account</li> <li>90 days service usage</li> </ul> <strong>Others</strong> <ul> <li>Amazon DevPay and FPS – for paying</li> <li>It’s always best practice to grant users access via IAM roles and groups even if they only need access once</li> <li>SWF has a gurantee that processes are only executed once against SQS</li> <li>Availability Zones offer you the ability to operate production applications and databases which are more highly available, fault tolerant and scalable than would be possible from a single data center.</li> <li>You can use <strong>AWS Config</strong> to continuously record configurations changes to Amazon RDS DB Instances, DB Subnet Groups, DB Snapshots, DB Security Groups, and Event Subscriptions and receive notification of changes through Amazon Simple Notification Service (SNS).</li> <li>SSD volumes must be between 1 GiB – 16 TiB.</li> <li>Economies of scale: The AWS Well-Architected framework has been developed to help cloud architects build the most secure, high-performing, resilient, and efficient infrastructure possible for their applications. This framework provides a consistent approach to application and solution architecture that will scale with your needs over time.</li> <li>AWS Config – enables you to keep track of all the config you have for your rersources</li> <li>In <b>cloud</b> computing, <b>elasticity</b> is defined as “the degree to which a system is able to adapt to workload changes by provisioning and de-provisioning resources in an autonomic manner, such that at each point in time the available resources match the current demand as closely as possible<strong>“</strong></li> <li><strong>Paying account and linked account for Consolidated billing</strong></li> </ul> <strong>Trusted Advisor </strong> <ul> <li><strong>Covers Performance, cost optimization, security and fault tolerance</strong></li> </ul>"
  },
  {
    "id": "31",
    "data": "<ul> <li><strong>Glacier</strong> – 10 gb free retrieval under free tier</li> <li><strong>Instance stores</strong> – cannot be in stopped state; they are either terminated or running</li> <li><strong>EC2 Instance states lifecycle:</strong> <ul> <li>pending</li> <li>running</li> <li>rebooting</li> <li>stopping</li> <li>stopped</li> <li>shutting-down</li> <li>terminated</li> </ul> </li> <li><strong>Cloudfront request if files is not in cache:</strong> holds the request until origin server serves it in the cache of the edge location</li> <li><strong>Cloudformation parameters:</strong> <ul> <li>Outputs: for specifying the outputs</li> <li>Parameters :(name etc)</li> <li>Mappings</li> <li>Resources: resources of cloudformation, type of resources</li> </ul> </li> <li><strong>Multi AZ RDS</strong> helps in maintenance tasks as it will initiate auto failover in standby</li> <li><strong>AWS VPC Platforms</strong> <ul> <li>EC2 Classic</li> <li>EC2-VPC</li> </ul> </li> <li><strong>RDS Soap webservices calls uses HTTPS only</strong></li> <li><strong>Lambda</strong> supports <ul> <li>Java</li> <li>Node.js</li> <li>Python</li> <li>C#</li> </ul> </li> <li>RDS Read Replicas limit: 5</li> <li><strong>AWS CodePipeline</strong> is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software.</li> <li><strong>CloudTrail logs to S3 bucket</strong> = 5minutes</li> <li><strong>Oracle BYOL (</strong>bring your own license): Standard Edition 2 (SE2), SE1, SE, Enterprise Edition (EE) <ul> <li>Included license: SE1, SE2</li> </ul> </li> <li><strong><a href='http://status.aws.amazon.com/' rel='nofollow'>http://status.aws.amazon.com</a> = AWS Service Health Dashboard</strong></li> <li><strong>Resources that can’t be tagged:</strong> <ul> <li>Dedicated hosts</li> <li>Elastic IP</li> <li>Instance Store volumes</li> <li>Key-Pair</li> <li>NAT GW</li> <li>Placement groups</li> <li>VPC endpoint/flowlog</li> </ul> </li> </ul>"
  },
  {
    "id": "32",
    "data": "<ul> <li><strong>Glacier</strong> – 10 gb free retrieval under free tier</li> <li><strong>Instance stores</strong> – cannot be in stopped state; they are either terminated or running</li> <li><strong>EC2 Instance states lifecycle:</strong> <ul> <li>pending</li> <li>running</li> <li>rebooting</li> <li>stopping</li> <li>stopped</li> <li>shutting-down</li> <li>terminated</li> </ul> </li> <li><strong>Cloudfront request if files is not in cache:</strong> holds the request until origin server serves it in the cache of the edge location</li> <li><strong>Cloudformation parameters:</strong> <ul> <li>Outputs: for specifying the outputs</li> <li>Parameters :(name etc)</li> <li>Mappings</li> <li>Resources: resources of cloudformation, type of resources</li> </ul> </li> <li><strong>Multi AZ RDS</strong> helps in maintenance tasks as it will initiate auto failover in standby</li> <li><strong>AWS VPC Platforms</strong> <ul> <li>EC2 Classic</li> <li>EC2-VPC</li> </ul> </li> <li><strong>RDS Soap webservices calls uses HTTPS only</strong></li> <li><strong>Lambda</strong> supports <ul> <li>Java</li> <li>Node.js</li> <li>Python</li> <li>C#</li> </ul> </li> <li>RDS Read Replicas limit: 5</li> <li><strong>AWS CodePipeline</strong> is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software.</li> <li><strong>CloudTrail logs to S3 bucket</strong> = 5minutes</li> <li><strong>Oracle BYOL (</strong>bring your own license): Standard Edition 2 (SE2), SE1, SE, Enterprise Edition (EE) <ul> <li>Included license: SE1, SE2</li> </ul> </li> <li><strong><a href='http://status.aws.amazon.com/' rel='nofollow'>http://status.aws.amazon.com</a> = AWS Service Health Dashboard</strong></li> <li><strong>Resources that can’t be tagged:</strong> <ul> <li>Dedicated hosts</li> <li>Elastic IP</li> <li>Instance Store volumes</li> <li>Key-Pair</li> <li>NAT GW</li> <li>Placement groups</li> <li>VPC endpoint/flowlog</li> </ul> </li> </ul>Amazon Web Services Soft Limits – they are the limits by default but this can be increased by sending a request to Amazon <p class='p1'><strong><span class='s1'>IAM</span></strong></p> <ul> <li class='p1'><span class='s1'>100 Groups </span></li> <li class='p1'><span class='s1'>250 Roles</span></li> <li class='p1'>5000 users</li> </ul> <strong>RDS</strong> <ul> <li>40 DB instance</li> <li>100 TB Total Storage</li> <li>35 days maximum backup</li> <li>DynamoDB <ul> <li>400kb item limit size</li> <li>1 read unit – 4kb</li> <li>1 write unit – 1kb</li> <li>Batch Operations – 16mb</li> <li>3 geographic locations</li> <li>Scan maximum – 1mb <ul> <li>local secondary index – size of all indexed items = 10gb or less</li> </ul> </li> </ul> </li> <li>Aurora DB <ul> <li>retains 6 copies (3 Az with 2 copies)</li> <li></li> </ul> </li> </ul> <strong>S3</strong> <ul> <li>100 Buckets per account</li> <li>5 TB size limit per bucket</li> </ul> <p class='p1'><strong><span class='s1'>EC2:</span></strong></p> <ul> <li class='p1'>50 instances per region</li> <li class='p1'><span class='s1'>500 Security group</span></li> <li class='p1'><span class='s1'>100 Rules per security group</span></li> <li class='p1'><span class='s1'>5000 key pairs</span></li> </ul> <strong>VPC</strong>: <ul> <li>5 Elastic IP</li> <li>5 VPC per region</li> <li class='p1'><span class='s1'>5 internet gateway</span></li> <li class='p1'><span class='s1'>5 NAT Gateways</span></li> <li class='p1'><span class='s1'>50 customer gateway per region</span></li> <li class='p1'><span class='s1'>50 VPN connection</span></li> <li class='p1'><span class='s1'>50 outbound/inbound per group</span></li> <li class='p1'><span class='s1'>200 subnet</span></li> <li class='p1'><span class='s1'>200 routes per table</span></li> </ul> <strong>CloudWatch</strong> <ul> <li>Store metrics after deletion: 2 weeks.</li> </ul> <strong>Regions</strong> <ul> <li>There are west-east and central regions</li> </ul> <strong>AWS Support Service Level</strong> <ul> <li>Basic</li> <li>Developer – non critical – 12 hrs, general – 24 hrs</li> <li>Business – Business impaired issues : 4 hour response time, business down – 1 hour response</li> <li>Enterprise – Critical problem : 15 minutes response time</li> </ul> <strong>Underlying AWS you can Access:</strong> <ul> <li>Elastic Map Reduce</li> <li>Elastic Beanstalk</li> <li>OpsWork</li> <li>EC2</li> </ul>"
  },
  {
    "id": "33",
    "data": "5 Pillars of Well Architected Framework <h3><strong>Security</strong></h3> Design Principles: <ul> <li>Apply Security at all layers</li> <li>Enable traceability</li> <li>Implement principle of least privilege</li> <li>Focus on securing your System (Data, Operating System, Application)</li> <li>Automate Security Events</li> </ul> Definition: <ul> <li>Use Detective Controls</li> <li>Infrastructure protection</li> <li>Data Protection</li> <li>Incident Response</li> </ul> Questions to ask for this pillar <ul> <li>How are you protecting access to and use of the root account credentials?</li> <li>How are you defining roles and responsibilities of system users to control human access?</li> <li>How are limiting automated access to your resources?</li> <li>How are you enforcing network and host-level boundary protection</li> <li>How are you protecting integrity of your operating systems</li> <li>How are you classifying data</li> <li>How are you encrypting and protecting your data at rest and at transit</li> <li>How are managing encryption keys</li> </ul> <h3><strong>Reliability</strong></h3> Design Principles <ul> <li>Test recovery procedures</li> <li>Automatically recover from failures</li> <li>Scale horizontally for more availability</li> <li>Stop guessing capacity needs</li> <li>Manage change in automation</li> </ul> Definition: <ul> <li>Ensure you have stable and correct foundations</li> <li>Change management</li> <li>Failure management</li> </ul> Questions to ask for this pillar <ul> <li>How are you managing service limits</li> <li>How are you planning network topology</li> <li>How does system adapt to change on demand</li> <li>How are you monitoring resources</li> <li>How are you executing change</li> <li>How are you backing up data</li> <li>How do you withstand component failures</li> <li>How are you testing for resiliency</li> <li>How are you planning for disaster recovery</li> </ul> <h3><strong>Performance</strong></h3> Design Principles <ul> <li>Consume advanced technologies – instead of asking your team to learn for new advance tech, consume these techs as services i.e: Machine Learning, Media transcoding</li> <li>Go global in minutes</li> <li>Use serverless architecture</li> <li>Experiment often</li> <li>Mechanical sympathy – use hardware that suit you needs</li> </ul> Definition <ul> <li>Selection – select correct resource for your needs <ul> <li>Compute</li> <li>Storage</li> <li>Database</li> <li>Network</li> </ul> </li> <li>Monitoring</li> <li>Tradeoffs – cost vs performance, etc</li> </ul> Questions to ask <ul> <li>How do you select best performing architecture</li> <li>How do ensure that you continue to have appropriate resources</li> <li>How do you monitor resources and ensure they are performing as expected</li> <li>How do you use tradeoffs to improve performance</li> </ul> <h3><strong>Cost Optimization</strong></h3> <u>Design Principles</u> <ul> <li>Adopt a consumption model – pay for what you use</li> <li>Benefit from economies of scale – buy more = more discount</li> <li>Stop spending on data centers</li> <li>Analyse expenditures</li> <li>Use managed services to reduce cost</li> </ul> Definition <ul> <li>Cost effective resource</li> <li>Match supply and demand</li> <li>Expenditure awareness</li> <li>Optimize over time</li> </ul> Questions to ask for this pillar <ul> <li>Do you consider cost when selecting resources</li> <li>Is your resources sized to meet cost target</li> <li>Have you selected appropriate pricing model</li> <li>Did consider data transfer cost</li> <li>How are you monitoring expenditures</li> <li>Are you stopping or do you decommission resources you don’t need</li> <li>Do you govern resources usage</li> <li>How do you manage adoption of new services</li> </ul> <h3><strong>Operational Excellence</strong></h3> Design Principles <ul> <li>Perform operations with codes</li> <li>Align process with business objectives</li> <li>Make regular, small, incremental changes</li> <li>Test responses for unexpected events</li> <li>Learn from operation events and failures</li> </ul> Definition <ul> <li>Preparation</li> <li>Operation</li> <li>Responses</li> </ul> Questions to ask <ul> <li>What best practices are you using</li> <li>How are you managing workload</li> <li>How are you evolving your workload with minimal impact of change</li> <li>How do you monitor workload</li> <li>How do you respond to unexpected events</li> <li>How is escalation managed if unexpected events occur</li> </ul>"
  },
  {
    "id": "35",
    "data": "<strong>What is Docker? </strong> <blockquote>'Docker is the world’s leading software container platform. Developers use Docker to eliminate “works on my machine” problems when collaborating on code with co-workers. Operators use Docker to run and manage apps side-by-side in isolated containers to get better compute density. Enterprises use Docker to build agile software delivery pipelines to ship new features faster, more securely and with confidence for both Linux and Windows Server apps.'</blockquote> https://www.docker.com/what-docker Basically, it is a container for your applications, you setup your docker with all the prerequisites for your web-applications and then you deploy the whole container. Say for example, you web-application needs to connect to mysql, or a messaging queue(e.g kafka), you setup docker once, and that same container you publish in different environments. It will be clearer once you we dive in to the details: <strong>Setting up docker.</strong> 1.) <strong>You need to install docker on your machine</strong>, download the necessary installer: <a href='https://docs.docker.com/docker-for-windows/install/'>https://docs.docker.com/docker-for-windows/install/</a> For windows home: <a href='https://www.docker.com/products/docker-toolbox'>https://www.docker.com/products/docker-toolbox</a> Note: Docker is different if you are just using windows home, windows pro enables you to virtualization hence docker is can run smoothly, for those os that does not support virtualization use <a href='https://www.docker.com/products/docker-toolbox'>docker-toolbox</a> 2.) <strong>Create an account in <a href='https://hub.docker.com/'>dockerhub</a></strong>, docker hub is like dockers repository, this is where finished images/repository are published, and are just pulled on the fly to your container. For example, if you want mysql in your container you would just pull from this one: <a href='https://hub.docker.com/_/mysql/'>dockerhub mysql</a> and then tweak it to your configuration. Once installed, run docker/docker-toolbox and you should be able to see this screen, note the ip:<strong> 192.168.99.100</strong> This ip acts as your <strong>localhost</strong> or <strong>127.0.0.1</strong>, so any applications running are accessible via this ip <strong><img class='alignnone size-full wp-image-328' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/05/ss-2.png' alt='' width='686' height='341' /></strong> 3.) <strong>Login into docker hub</strong> in your docker application via running the below: <strong>docker login </strong> <img class='alignnone size-full wp-image-329' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/05/ss-3.png' alt='' width='941' height='104' /> I have mine already logged in, but you should see something similar. 4.) <strong>Pull hello-world image via this command, </strong>if you are wondering where we got hello-world, it is located in <a href='https://hub.docker.com/r/tutum/hello-world/'>docker-hub tutum/hello-world.</a> We are just pulling a image from docker-hub. <strong>docker pull tutum/hello-world</strong> <img class='alignnone size-full wp-image-331' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/05/ss1.png' alt='' width='642' height='183' /> 4.) Run the actual image, using this command: <strong>docker run -d -p 8080:80 --name first_example tutum/hello-world</strong> The flags are explained below <ul> <li><strong>run</strong> - docker run command</li> <li><strong>-d</strong> - run in background</li> <li><strong>-p</strong> - publish to port 80, (THIS IS VERY IMPORTANT, since we are running in our docker, we need to be able publish this outside our container otherwise we won't be able to hit the application from outside docker.</li> <li><strong>--name</strong> : pass the name of container</li> <li><strong>tutum/hello-world</strong> - lastly, the image we want to run</li> </ul> Use this command to see the running instances : <strong>docker ps</strong> <strong><img class='alignnone size-full wp-image-332' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/05/ss2-1.png' alt='' width='1174' height='143' /></strong> notice that it says ports: 0.0.0.0/8080 -&gt; 80 tcp, this is the port we exposed to so it should be reachable now via: http://192.168.99.100:8080/ <img class='alignnone size-full wp-image-333' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/05/ss1-1.png' alt='' width='1233' height='339' /> *Yes that is the docker ip! if we don't specify -p flag when running we won't be able to hit this in our local browsers. Congrats you have now started your own docker container! Check out this page for command references, which are pretty straight-forward: <a href='https://docs.docker.com/engine/reference/commandline/docker/'> https://docs.docker.com/engine/reference/commandline/docker/</a>"
  },
  {
    "id": "36",
    "data": "Below are more detailed notes on various aspects of Docker, and somewhat more advanced topics with dealing with container/image management. Docker commands are now separated into main modules. previously when you wanted to run a command, you would do docker &lt;command&gt; In the newer version you can say: docker &lt;module&gt; &lt;module_command&gt; <strong>Modules</strong> <ul> <li>config - Manage Docker configs</li> <li>container - Manage containers</li> <li>image - Manage images</li> <li>network - Manage networks</li> <li>node - Manage Swarm nodes</li> <li>plugin - Manage plugins</li> <li>secret - Manage Docker secrets</li> <li>service - Manage services</li> <li>stack - Manage Docker stacks</li> <li>swarm - Manage Swarm</li> <li>system - Manage Docker</li> <li>volume - Manage volumes</li> </ul> <strong>Run vs Start</strong> <ul> <li>run - run will always create a new container</li> <li>start - will start an existing one</li> </ul> <strong>Images vs Container</strong> <ul> <li>Images are template that are collected from dockerhub</li> <li>container is the running instance <ul> <li>you can have multiple containers based on one image</li> </ul> </li> </ul> <strong>Note:</strong> you can terminate docker by just specifying of container id for example our container id is 1234567, you can stop the container by: <em>docker container stop 123</em> Common Docker local ip is usually: 192.168.99.100 <strong>Performance Monitoring: </strong><em>docker container stats</em> SSH inside a server would be: <ul> <li><em>docker exec -it nginx</em></li> <li><em>docker container exec -it &lt;container_name&gt; bash/sh/etc</em></li> </ul> -i : interactive -t : pseudo TTY <em>linux alpine</em> - small secure linux box <strong>Networking</strong> Common commands: <ul> <li>docker network ls</li> <li>docker network create</li> <li>docker network connect</li> </ul> You can link your containers to a specific network via the flag: <em>--network &lt;network_name&gt;</em> <em>link container via network:</em> docker container run -d --net &lt;network_name&gt; --net-alias &lt;network-alias&gt; &lt;container&gt; <strong>TAGGING and PUSHING, CACHING LAYERS</strong> <strong>Logging - </strong>docker already handles the logs, you just need to make sure it goes to stdout or stderr, current trend is leaning to stdout rather than log4j or any other logging frameworks <strong>Dockerfile Essentials Format</strong> FROM &lt;IMAGE&gt; WORKDIR &lt;Directory&gt; - THIS IS a CD or Change Directory Command ENV &lt;environment&gt; RUN &lt;RUN COMMANDS&gt; EXPOSE &lt;EXPOSE PORT&gt; CMD &lt;RUN COMMAND EVERYTIME CONTAINER IS LAUNCHED&gt; COPY &lt;from&gt; &lt;to&gt; - copy local to docker <strong>Image building</strong>: <em>docker image build -t &lt;tag&gt; &lt;location&gt;</em> <em>NOTE:</em> that building the image does not mean you are running it, container and image are totally different <strong>Named volume format:</strong> -v &lt;volume_name&gt;:&lt;container_volume&gt; <strong>BIND MOUNT (CANT USE IN DOCKERFILE)</strong> -v full mount : &lt;container_volume&gt; <strong>Notes/Gotchas:</strong> <ul> <li>VOLUMES on windows docker toolbox can only be accessible through c:/users via /c/users/</li> <li>IF YOU HAVE AN EXISTING IMAGE, you will need to rebuild that image, know that -volumes does not work on dockerfile</li> <li>in contrast if you want to just run a DockerFile you would do docker build</li> </ul> <em>docker-compose up</em> - to build images for development <em>docker-compose build or up --build</em> - to rebuild images, only given that you have a build tag in your yml file <strong>Docker Swarm</strong> <ul> <li><em>docker swarm init</em> - creates raft database of root ca,configs secrets - PKI, security automation</li> <li><em>docker node ls</em> - check nodes</li> <li><em>docker service create</em> - create a service part of the swarm</li> <li><em>docker service ls</em> - list of services</li> <li><em>docker service update</em> - update existing service with regards to orchestration</li> <li><em>docker service rm heuristic_shirley</em> - to actually delete service from docker service</li> </ul> <strong>CREATING RANCHER MACHINE:</strong> docker-machine create dockermachine --driver virtualbox --virtualbox-cpu-count '-1' --virtualbox-disk-size '8000' --virtualbox-memory '1024' --virtualbox-boot2docker-url=https://github.com/boot2docker/boot2docker/releases/download/v1.10.3/boot2docker.iso docker-machine create dockermachine --driver virtualbox --virtualbox-cpu-count '-1' --virtualbox-disk-size '8000' --virtualbox-memory '1024'"
  },
  {
    "id": "37",
    "data": "On all our previous example we have been using <em><span style='text-decoration: underline;'><strong>Tell</strong></span></em> method to send messages to actors, in Akka this is known as fire-and-forget. Which means you send a message and there is no blocking, Actors, handle a separate thread to handle the actual work. But what happens when you need a return value? you pass a message and some time down the line you want to know what is the response. Here is where <em><strong>Ask</strong></em> comes in to play: <img class='alignnone size-full wp-image-400' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/08/ss-7.png' alt='' width='1163' height='386' /> Ask makes use of Java's <strong>completable</strong> future, to see that when do the response does come back, we can then do some things to it. On the <strong>Actor</strong> side, we make use of <strong>getSender.tell</strong> to return the actual message to CompletableFuture object."
  },
  {
    "id": "38",
    "data": "By default Akka does not make your variables immutables, so if you are passing around message/objects with mutable variables there is still a case where you can experience a deadlock. That is why it is highly recommended to use immutable messages when integrating with Akka Actors. What are immutable messages? Basically these are variables that cannot be changed any more. In Java jargon these are final variables. A simple example here is this <strong>SampleMessage</strong> class file, in this case we only have one variable which is <strong><em>final </em>message</strong>, and can only be set on constructor of the class and can no longer be changed. <img class='alignnone size-full wp-image-392' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/08/ss-5.png' alt='' width='396' height='190' /> Let's see a more detailed example: <img class='alignnone size-full wp-image-393' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/08/ss-6.png' alt='' width='1296' height='619' /> Code breakdown: Line 12 - create an <em><strong>actorSystem</strong> </em>with 'messages' as the name Line 13 - Creates an actual <em><strong>Actor</strong> </em>through <span style='text-decoration: underline;'><em><strong>ActorRef</strong></em></span>, through <em><strong>Props</strong></em>, we pass two parameters, see Line 19, in this case, it could be a database connection or whatever references you want to pass to the Actor. Line 15 - we <span style='text-decoration: underline;'><em><strong>tell </strong></em></span>the actor to handle SampleMessage class Line 18 - we create the Actor by extend <em><span style='text-decoration: underline;'><strong>AbstractActor</strong> </span></em>and handling <em><strong>createReceive</strong> </em>Method, via <span style='text-decoration: underline;'><em><strong>ReceiveBuilder</strong></em></span>, in laymans terms we are declaring that if the Actor param is a SampleMessage.class then call handleMessage method. Line 30 - we have the <strong>immutable</strong> SampleMessage class <strong>Note: </strong>You shouldn't create your whole messages via constructor if you have a lot of parameters, say you have PersonObject with alot of variables like name, age, address, etc. Use a external library like Immutables (https://immutables.github.io/) to generate an object with final references."
  },
  {
    "id": "39",
    "data": "<strong>Akka Actor Model </strong> Dealing with highconcurrency, highly scalable applications. Again, just want to emphasize this site does not aim to give a complete comprehension of given topic, but rather an overview, or review of certain topics. It is still recommended to do a reading on the actual documentation. <strong>Recommended Reading:</strong> http://doc.akka.io/docs/akka/current/java/guide/tutorial_1.html <strong>What is Actor Architecture? </strong>In a very oversimplified explanation, it is framework to handle highly concurrent applications. Actor aims to eliminate the call stack problem: <ul> <li>ProcessA calls a method on ProcessB</li> <li>ProcessB has a very long method that takes more than 5 seconds to complete</li> <li>ProcessA is just waiting for ProcessB to end</li> <li>ProcessB ends and returns a result</li> <li>ProcessA resumes its operations</li> </ul> <img class='alignnone size-full wp-image-379' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/08/ss-1.png' alt='' width='561' height='483' /> (Photo reference: http://doc.akka.io/docs/akka/current/java/guide/actors-motivation.html#the-illusion-of-a-call-stack) With Actor Architecture, Actors are the processes and they just pass around messages, in the scenario above, ProcessA will just the task to ProcessB and continue on. With that said, lets dive in. Here are some basic jargons/requirements for Actor Architecture: <ol> <li><strong>AbstractActor </strong>-classes require to extend AbstractActor to act as an Actor</li> <li><strong>Receive</strong> -method to map method signature ( method from AbstractActor)</li> <li><strong>Props</strong> - configuration class to specify options of creation of actors</li> <li><strong>ActorSystem</strong> - is similar to Spring context, manages lifecycles of Actors</li> <li><strong>tell </strong>- fire and forget mechanism of Actors</li> </ol> Basic Hello world using Actors: <img class='alignnone size-full wp-image-380' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/08/ss-2.png' alt='' width='997' height='388' /> Breaking down the codes: ActorSystem, creates an hierarchy of your actors that are under whatever name you give it. Props creates an instance of DemoActor, with possible configuration (none in this case) DemoActor extends Abstract Actor, with the required method. RecieveBuilder, creates a Receive for you, which really is just defining the parameters signature of your Actor. In this case if you can accept a parameter String.class, which in turn will just print out 'Demoing + parameter' Your output in the code above will result in the below, notice that 'Am I Blocking' is written first even though it is written in the codes last, it is because actors are not blocking, we fired a request to demoactor and the execution continued. <blockquote>Am I Blocking? Demoing This is the message sent</blockquote>"
  },
  {
    "id": "40",
    "data": "Akka Architecture by default supports parent-child Actor structures. On startup Akka already creates several actors for you: <img class='alignnone size-full wp-image-387' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/08/ss-3.png' alt='' width='842' height='491' /> http://doc.akka.io/docs/akka/current/java/guide/tutorial_1.html <ul> <li>Root guardian - to handle all the actors</li> <li>System guardian - Akka System guardian</li> <li>User guardian - user in this case is you, user guardian manages all the actors created by the developer.</li> <li>Usually you can only create Actors under /user guardian, and then you can further nest by creating actors under your actors.</li> </ul> To further the previous example, our DemoActor now has a ChildDemoActor. Actor Principle is retained here ChildDemoActor tell command is non blocking, so should you have complex things to do parent actor, child actor is also concurrently doing it's own thing. <img class='alignnone size-full wp-image-388' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/08/ss-4.png' alt='' width='827' height='510' /> Also the value here is, Parent manages the child actors, should the parent actor be terminated, it will terminate all its child actors first. this helps manages actors more efficiently."
  },
  {
    "id": "41",
    "data": "This is not suppose to be a comprehensive guide on how to use React JS, but more of a cheat sheet if you need a refresher on how react works. <strong>React Key concepts</strong> <ul> <li><strong>components structured</strong> - Files are separated in what they call components, and each part of the page would have their own js files, for example a page with youtube videos can be broken down like so:</li> <li>each js would require an <strong>export </strong>to determine which will be used by the main js. for example, search bar would need to export an input type so that the main app js when he imports search_bar he can make use of a input type that is the search bar</li> </ul> <img class='alignnone size-full wp-image-342' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/07/ss.png' alt='' width='236' height='163' /> <strong>Class Based component:</strong> <ul> <li>we use class based components, if we want a component to maintain some sort of state</li> <li>Note that &lt;<strong>class&gt;</strong> <strong>extends Component</strong></li> <li>Constructor is the class constructor</li> <li>render is what is actually displayed</li> <li>note the <strong>export default</strong> at the bottom</li> </ul> <img class='alignnone size-full wp-image-343' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/07/ss-1.png' alt='' width='473' height='368' /> <strong>Functional Components</strong> <ul> <li>are used for simple components that are used for just displaying, and no need to keep track of state.</li> <li>for example the below, just takes a parameter and displays them as a list &lt;ul&gt;</li> </ul> <img class='alignnone size-full wp-image-344' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/07/ss-2.png' alt='' width='588' height='312' /> <strong>Component-level State</strong> Class state are a similar concept to object oriented programming, whereas a class has instance variables (state) and they are initialized in the constructor and maintained by the class. <em>Note that they are only component level state, and the changes are only reflected locally on that js</em> For example, the below initializes a couple of variables like <em>results, arrayNum, selectedNum, query</em> <img class='alignnone size-full wp-image-346' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/07/ss-4.png' alt='' width='296' height='181' /> <em>True value of React JS is when state is altered, it automatically re-renders the page or it <strong>reacts</strong> to it.</em> <strong>Importing syntax</strong> when importing from a library provided by react, you can just implicitly say: <em>import React from 'react'; </em> where 'react' is the library provided and React is the module you want to import. However if importing from user created js files like importing a search bar from an external js, you have to explicitly state which folder that js is located, see below: <strong><img class='alignnone size-full wp-image-347' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/07/ss-5.png' alt='' width='369' height='171' /></strong> <strong>Function Callback</strong> In react you can pass methods to be used by other js. This is a more complex way of passing method, and should be answered really by redux. by for completeness sake, if you are using only react. for example the below, note the <strong>valueSearch</strong> method, we declare it in the Main App component, because we want to set the state to change if say the user performs a search. <img class='alignnone size-full wp-image-348' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/07/ss-6.png' alt='' width='483' height='413' /> However, search bar is in a different component other than main app component, hence we passed the <strong>valueSearch</strong> method via: <em>&lt;SearchBar <strong>onSearchBarQuery</strong>={query =&gt; this.<strong>valueSearch</strong>(query)}/&gt;</em> <em><strong>onSearchBarQuery </strong>- is now the new method name in search bar js, which actually just performs the valueSearch.</em> So if we check search bar again, you have an input type that has a call changeState <strong>onChange,</strong> which actually performs <strong>onSearchBarQuery</strong> which in turn calls <strong>valueSearch</strong> <img class='alignnone size-full wp-image-343' src='http://sysdotoutdotprint.com/wp-content/uploads/2017/07/ss-1.png' alt='' width='473' height='368' />"
  },
  {
    "id": "42",
    "data": "<strong>Redux Notes:</strong> <ul> <li>Redux is in charge of the whole application state NOT component state</li> <li>All Reducers are matched in combineReducers method</li> <li>Action Creator are in charge of changing application state</li> </ul> <strong>Store</strong> <ul> <li>Action Creator - returns an object action, which automatically sent to all reducers</li> <li>reducer_data - is your data</li> </ul> <strong>Some Imports to note</strong> <span style='font-family: Consolas, Monaco, monospace;'>import {connect} from 'react-redux';</span> <ul> <li>allows you to mapstate to props so you wire up the reducer data to the props binding Action Creators syntax</li> <li>the difference with a normal function is that it will pass to all reducers</li> </ul> <pre>import {bindActionCreators} from 'redux';</pre> <ul> <li>for binding action creators this will be part of props as well</li> </ul> <pre><strong>Some Functions </strong> function mapDispatchToProps(dispatch){ return bindActionCreators({selectName : selectName}, dispatch); } is the same as: this.props.selectName export function selectName(name){ console.log('this name has been selected ' + name.name return{ type: 'BOOK_SELECTED', payload: name }; }</pre> <strong>NOTES</strong> When creating a new js, know first whether is the a COMPONENT OR A CONTAINER <ul> <li><strong>COMPONENT</strong> is just a react file, it just cares with displaying, no state whatsoever</li> <li><strong>CONTAINER</strong> cares about state, and is redux, needs to extend Component</li> <li><strong>Action</strong> are just handlers of action done</li> <li><strong>Reducers</strong> are what to do with the action done</li> <li><strong>Middleware</strong> are interceptors,they process promises to results <ul> <li>This is especially useful when dealing with asynchronous calls</li> <li>when we talk about promise we are dealing with:</li> </ul> </li> </ul> <pre>import promise from 'redux-promise';</pre> <strong>Pulling data from reducer to Container:</strong> <ol> <li> <pre> import connect from {react-redux}</pre> </li> <li> <pre> function mapStateToProps(state){ // parameter state return {data: state.data}}</pre> </li> <li> <pre> export default connect(mapStateToProps)(className)</pre> </li> </ol> <strong>Routes</strong>: using routes to manipulate pages via URL redirecting <strong>Note</strong> that React/Redux really is a SPA (Single Page Application) but the Routes determine what components to display making it seem like there are multiple pages <pre> &lt;Route path='/Posts/new' component={PostsNewPage}/&gt;</pre> <pre>React bootsrap css: &lt;link rel='stylesheet' href='https://maxcdn.bootstrapcdn.com/bootstrap/latest/css/bootstrap.min.css'&gt;</pre> <strong>Redux-form - </strong>Form validation and form submission in Redux <ul> <li> <pre>import {reducer as formReducer} from 'redux-form'; ... form: formReducer</pre> </li> <li> <pre>import {Field, reduxForm} from 'redux-form';</pre> </li> </ul> <strong>Creating Fields:</strong> <pre>&lt;Field label='Title' name='title' type='text' component={this.renderField} /&gt; renderField(field){ return(&lt;div className='form-group'&gt; &lt;label&gt;{field.label}&lt;/label&gt; &lt;input className='form-control' type={field.type} {...field.input}/&gt; &lt;/div&gt;)} export default reduxForm({ form: 'PostsNew' })(PostsNew)</pre> <strong>connecting redux form to action controller:</strong> <pre>export default reduxForm({ validate, form: 'PostsNewForm' })( connect (null, {savePosts})(PostsNew) );</pre> <strong>Notes: this.props.match.params.id</strong> - to get params on URL from REACT ROUTER<strong> </strong> <strong>MAP STATE TO PROPS</strong> - Props is declared in root combineReducers, so if you name an object in root reducer as postProps is declared in root combineReducers, so if you name an object in root reducer as post then it can be access in map state to props as post"
  },
  {
    "id": "43",
    "data": "<strong>Cassandra</strong> are non-relational/NoSQL is a highly scalable, high-performance distributed database designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure. Cassandra used by giants like NetFlix, eBay, Twitter and Reddit. <strong>Snitches </strong> Determines how Cassandra can tell the topology of the infrastructure. In layman's term how nodes would talk to each other. <strong>Types of Snitches</strong> <ol> <li>Dynamic</li> <li>Simple</li> <li>Rack Inferring</li> <li>Property File</li> <li>Gossiping Property File</li> <li>Ec2</li> <li>Ec2 MultiRegion</li> </ol> By Default, for nodes to work each nodes should have same snitch type, for example: <strong>PropetyFileSnitch Example</strong> Node1: 130.77.100.147=&lt;DataCenter&gt;:&lt;Rack&gt; Node2: 130.77.100.148=&lt;DataCenter&gt;:&lt;Rack&gt; Node3: 130.77.100.149=&lt;DataCenter&gt;:&lt;Rack&gt; <em>Con for propertyFileSnitch</em>: very tedious as you need to update manually for each node <h2><strong>Key Concepts</strong></h2> <strong>Internal communication</strong> : Gossip - is how to node communicate with each other - runs every 1 second <strong>External Communication</strong> : CQL / Thrift <strong>Data distribution</strong> <ul> <li>data rows are distributed</li> <li>partitioner is used to determine distribution</li> <li>Murmur3 is default partitioner, it determine a hash to which node to distribute toEach node has a hash determined to handle which set of hash to handle</li> <li>You can use murmur3 calculator to determine hash</li> </ul> <strong>Replication Factor</strong> - replication of data to different nodes for high availability <strong>Virtual Nodes</strong> - each node can have small token ranges (256 by default) <strong>main config file:</strong> cassandra.yaml <strong>partitioner</strong>: partioner type <strong>COMMANDS:</strong> <pre>CREATE KEYSPACE sampleWITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1};</pre> <pre>INSERT INTO sample.activity (id,description, datetime, code)VALUES ('1', 'test', '2017-10-10 01:01:01', 'CODE'); INSERT INTO sample.activity (id) VALUES ('2'); select * from sample.activity; CREATE TABLE sample.activity ( id text PRIMARY KEY, description text, datetime timestamp, code text); <strong>COPY FROM CSV command:</strong> copy activity (id, code, description) from '/var/lib/cassandra/import.csv' WITH header = false AND delimiter = '|';</pre> <strong>Docker side notes:</strong> docker exec -it &lt;container_name&gt; cqlsh - to run cqlsh from docker docker cp to copy files into container OR Just add in Dockerfile <strong>Data Storage:</strong> DATA is stored via partition key and can be viewed via cassandra-client: Partition key 1: events events events Partition key 2: events events events Data is stored on disk by both disk and in memory using memcache, then flushed to disk as SSTable <em><strong>NOTE</strong> WHERE CLAUSE GENERALLY NEED TO INCLUDE PARTITION KEY </em>can't query using other columns that are not partition key, you need secondary <em>indexes</em> <em><strong>CONTRARY TO SQL</strong> - SECONDARY INDEXES DOES NOT MAKE THE QUERY FASTER, INSTEAD IT CREATES A HIDDEN TABLE UNDERNEATH ANOTHER OPTION IS TO CREATE ANOTHER TABLE FOR SPECIFICALLY THAT QUERY ONLY.</em> <strong>Secondary INDEXES</strong> <ul> <li>PRO: you dont need to manually maintain</li> <li>CON: Slower, as it accesses all nodes</li> </ul> For example you have a activities table, with code column where code is not a partition if you want to query activities per code, you can create a table with <em><span style='text-decoration: underline;'>activities_per_code</span> </em>table, where you use code in this table as primary <ul> <li>PRO: you dont need to access all nodes when querying another table</li> <li>CON: harder to maintain</li> </ul> <pre><strong>HOW TO CREATE INDEX:</strong> CREATE INDEX &lt;index_name&gt; ON &lt;table&gt; (column_name);</pre> <ol> <li><strong>HARDWARE:</strong> minimum 8gb per node prod 32gb of ram per node</li> <li>minimum core 4 gb common 8gb core</li> <li>Disk: ( do NOT SHARE) SSD are prefered 500gb to 1tb commit log should be seperate</li> </ol> When updates are called, it just keeps adding to it then it determines which is the latest versions <strong>Command:</strong> <strong>SOURCE</strong>: executes a series of commands in a file <strong>DELETE</strong> command - delete a value from a row, or delete a whole row <strong>TRUNCATE/DELETE</strong> are as is <strong>DESCRIBE KEYSPACES</strong> When you delete a data, a tombstone is created, to allow nodes to replicate this gc_grace_seconds, 10 days by default <strong>Compaction</strong> is when is when delete is actually done, Compaction is when SStables are merged, can be triggered manually via nodetool command <strong>nodetool sstable2json -</strong> convert tables to json <strong>TTL</strong> for cleanup of data, you can specify ttl on insert data to automatically delete said datayou can update TTL value as well <pre>execute nodetool commands in docker, check node status: docker exec -it mel_cass sh -c 'nodetool status'</pre> <strong> NETWORKING </strong>ports: <ul> <li>7000 for gossiping</li> <li>9042: expose</li> <li>9160: thrift</li> <li>7199: JMX</li> </ul> <strong>Cassandra.yml</strong> <strong>listen_address</strong> - ip for other nodes to find <strong> rpc_address</strong> - ip for your app to find cassandra <strong>seeds</strong> - for declaring all the seeds in your cassandra <strong>bootstrap</strong> <ul> <li>adding more cluster</li> <li>auto-bootstrap should be true in cassandra.yml</li> <li>num_tokens - number of tokens to be handled by node</li> <li>cluster name should be the same</li> <li>you can just specify one or two seeds on one cluster and it will autodetect other nodes</li> </ul> When a node auto bootstraps, say node 1 has data ABC, and node 2 joins and should handle BCnode1 has still that data, you need a cleanup command to clean node 1 <strong>cleanup command</strong>:nodetool -h &lt;ip&gt; cleanup <strong>Cassandra-stress</strong> - used for stress-testing cassandra <strong>MONITORING</strong>: <strong>JConsole</strong> - Java UI <strong>Nodetool</strong> - cassandra console <strong>Datastax Ops Center</strong> - you need to install opscenter agent <strong>nodetool decommission</strong> <ul> <li>choose to decommission a node, tokens are assigned to other nodes, copies to other data</li> <li>node has still data, better have a clean node</li> <li>so clean the whole data first: rm -r commitlog data saved_caches</li> <li>nodetool removenode - so that tokens are reassinged to other nodes</li> <li>nodetool removenode &lt;id&gt;</li> </ul> repair node comes into play if you have more then a replication factor of 1, you need to repair in the following cases: node has been down replication factor of keyspace increased token ranges has been changed"
  }

]